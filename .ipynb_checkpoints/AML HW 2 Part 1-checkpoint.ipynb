{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef761591",
   "metadata": {},
   "source": [
    "# Homework 2 Binary Classification on Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed7ebc8",
   "metadata": {},
   "source": [
    "## Part a: Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c43fda68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "20e7cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "20a5a4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data points: 7613\n",
      "Test data points: 3263\n"
     ]
    }
   ],
   "source": [
    "print('Training data points:', len(train))\n",
    "print('Test data points:', len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac6c8f8",
   "metadata": {},
   "source": [
    "1) There are 7613 training data points and 3263 test data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d2fefd8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4296597924602653"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[train['target'] == 1]) / len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bb908310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5703402075397347"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[train['target'] == 0]) / len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd2156b",
   "metadata": {},
   "source": [
    "2) \\~43% of the training tweets are about real disasters and  \\~57% of the training tweets are about non-real disasters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1a1d23",
   "metadata": {},
   "source": [
    "## Part b: Split the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5219f13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3b1b2884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>686</td>\n",
       "      <td>attack</td>\n",
       "      <td>#UNITE THE BLUE</td>\n",
       "      <td>@blazerfan not everyone can see ignoranceshe i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4854</th>\n",
       "      <td>6913</td>\n",
       "      <td>mass%20murderer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>White people I know you worry tirelessly about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4270</th>\n",
       "      <td>6066</td>\n",
       "      <td>heat%20wave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chilli heat wave Doritos never fail!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>1441</td>\n",
       "      <td>body%20bagging</td>\n",
       "      <td>New Your</td>\n",
       "      <td>@BroseidonRex @dapurplesharpie I skimmed throu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4475</th>\n",
       "      <td>6365</td>\n",
       "      <td>hostages</td>\n",
       "      <td>cuba</td>\n",
       "      <td>#hot  C-130 specially modified to land in a st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4931</th>\n",
       "      <td>7025</td>\n",
       "      <td>mayhem</td>\n",
       "      <td>Manavadar, Gujarat</td>\n",
       "      <td>They are the real heroes... RIP Brave hearts.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3264</th>\n",
       "      <td>4689</td>\n",
       "      <td>engulfed</td>\n",
       "      <td>USA</td>\n",
       "      <td>Car engulfed in flames backs up traffic at Par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>2388</td>\n",
       "      <td>collapsed</td>\n",
       "      <td>Alexandria, Egypt.</td>\n",
       "      <td>Great British Bake Off's back and Dorret's cho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2607</th>\n",
       "      <td>3742</td>\n",
       "      <td>destroyed</td>\n",
       "      <td>USA</td>\n",
       "      <td>Black Eye 9: A space battle occurred at Star O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>3924</td>\n",
       "      <td>devastated</td>\n",
       "      <td>Dorset, UK</td>\n",
       "      <td>???????????? @MikeParrActor absolutely devasta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5329 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id          keyword            location  \\\n",
       "476    686           attack   #UNITE THE BLUE     \n",
       "4854  6913  mass%20murderer                 NaN   \n",
       "4270  6066      heat%20wave                 NaN   \n",
       "992   1441   body%20bagging            New Your   \n",
       "4475  6365         hostages                cuba   \n",
       "...    ...              ...                 ...   \n",
       "4931  7025           mayhem  Manavadar, Gujarat   \n",
       "3264  4689         engulfed                 USA   \n",
       "1653  2388        collapsed  Alexandria, Egypt.   \n",
       "2607  3742        destroyed                 USA   \n",
       "2732  3924       devastated          Dorset, UK   \n",
       "\n",
       "                                                   text  \n",
       "476   @blazerfan not everyone can see ignoranceshe i...  \n",
       "4854  White people I know you worry tirelessly about...  \n",
       "4270               Chilli heat wave Doritos never fail!  \n",
       "992   @BroseidonRex @dapurplesharpie I skimmed throu...  \n",
       "4475  #hot  C-130 specially modified to land in a st...  \n",
       "...                                                 ...  \n",
       "4931  They are the real heroes... RIP Brave hearts.....  \n",
       "3264  Car engulfed in flames backs up traffic at Par...  \n",
       "1653  Great British Bake Off's back and Dorret's cho...  \n",
       "2607  Black Eye 9: A space battle occurred at Star O...  \n",
       "2732  ???????????? @MikeParrActor absolutely devasta...  \n",
       "\n",
       "[5329 rows x 4 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X = list(train.columns[:-1])\n",
    "\n",
    "# Splits train.csv into training set (70%) and development set (30%)\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(train[X], train['target'], test_size=0.3, random_state=0)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f229cf",
   "metadata": {},
   "source": [
    "## Part c: Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fef4e9f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "476     @blazerfan not everyone can see ignoranceshe b...\n",
       "4854    White people I know you worry tirelessly about...\n",
       "4270                 Chilli heat wave Doritos never fail!\n",
       "992     @BroseidonRex @dapurplesharpie I skim through ...\n",
       "4475    #hot C-130 specially modify to land in a stadi...\n",
       "                              ...                        \n",
       "4931    They be the real heroes... RIP Brave hearts......\n",
       "3264    Car engulfed in flame back up traffic at Parle...\n",
       "1653    Great British Bake Off's back and Dorret's cho...\n",
       "2607    Black Eye 9: A space battle occur at Star O784...\n",
       "2732    ???????????? @MikeParrActor absolutely devasta...\n",
       "Name: text, Length: 5329, dtype: object"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet \n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# import nltk\n",
    "# nltk.download()\n",
    "\n",
    "# Lemmatize words based on part of speech (verbs, adjectives, and nouns)\n",
    "def lemmatize(text):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    word_tags = pos_tag(text.split())\n",
    "    result_text = []\n",
    "    for word_tag in word_tags:\n",
    "        lemmatized_word = word_tag[0]\n",
    "        # lemmatize verbs (e.g. ate -> eat)\n",
    "        if 'VB' in word_tag[1]:\n",
    "            lemmatized_word = wnl.lemmatize(word_tag[0], pos='v')\n",
    "        # lemmatize adjectives (e.g. better -> good)\n",
    "        elif 'JJ' in word_tag[1]:\n",
    "            lemmatized_word = wnl.lemmatize(word_tag[0], pos='a')\n",
    "        # lemmatize nouns (e.g. cookies -> cookie)\n",
    "        elif 'NN' in word_tag[1]:\n",
    "            lemmatized_word = wnl.lemmatize(word_tag[0], pos='n')\n",
    "        result_text.append(lemmatized_word)\n",
    "    return ' '.join(result_text)\n",
    "\n",
    "X_train['text'] = X_train['text'].apply(lambda text: lemmatize(text))\n",
    "X_dev['text'] = X_dev['text'].apply(lambda text: lemmatize(text))\n",
    "test['text'] = test['text'].apply(lambda text: lemmatize(text))\n",
    "X_train['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "92145aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "stop_words = ['the', 'a', 'an', 'and', 'or', 'this', 'that', 'i', 'my', 'me', 'we', 'us', 'our', 'she', 'her', \n",
    "              'he', 'his', 'him', 'they', 'their', 'them', 'you', 'your', 'there', 'are', 'is', 'from', 'to',\n",
    "              'will', 'can', 'cant', 'would', 'has', 'have', 'could', 'be', 'as', 'if', 'in', 'on', 'also', 'at', \n",
    "              'of', 'into', 'by', 'be', 'it', 'its', 'so', 'im', 'youre', 'theyre', 'hes', 'shes', 'were', 'was', \n",
    "              'not','but', 'no', 'never', 'with', 'really', 'do', 'for', 'about', 'what', 'how', 'who', 'just',\n",
    "              'when', 'via', 'which', 'than', 'like']\n",
    "\n",
    "def regex_stop_word(words):\n",
    "    regex = r'\\b'\n",
    "    for i in range(len(words)):\n",
    "        if i == len(words) - 1:\n",
    "            regex += words[i] + r'\\b'\n",
    "        else:\n",
    "            regex += words[i] + r'\\b|\\b'\n",
    "    return regex\n",
    "\n",
    "\n",
    "def preprocess_text(X):\n",
    "    # Converts all the words to lowercase\n",
    "    X = X.apply(lambda text: text.lower())\n",
    "    \n",
    "    # Removes URLs\n",
    "    X = X.apply(lambda text: re.sub(r'http\\S+', ' ', text))\n",
    "    \n",
    "    # Removes user id\n",
    "    X = X.apply(lambda text: re.sub(r'@(.*?)[\\s]', ' ', text))\n",
    "    \n",
    "    # Strips punctuations\n",
    "    X = X.apply(lambda text: re.sub(r'[^\\w\\s]', '', text))\n",
    "    \n",
    "    # Strips stop words\n",
    "    X = X.apply(lambda text: re.sub(regex_stop_word(stop_words), ' ', text))\n",
    "    return X\n",
    "\n",
    "X_train['text'] = preprocess_text(X_train['text'])\n",
    "X_dev['text'] = preprocess_text(X_dev['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1413c3f",
   "metadata": {},
   "source": [
    "## Part d: Bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ce8462e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "M = 10\n",
    "vectorizer = CountVectorizer(binary=True, min_df=M)\n",
    "vtz = vectorizer.fit(X_train['text'])\n",
    "V_train = vtz.transform(X_train['text']).toarray()\n",
    "V_dev = vtz.transform(X_dev['text']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e074b5",
   "metadata": {},
   "source": [
    "## Part e: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "eee88cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores for logistic regression model without regularization\n",
      "\tTraining data: 0.8345965225144895\n",
      "\tDevelopment data: 0.7204703367183325\n"
     ]
    }
   ],
   "source": [
    "# without regularization terms\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('F1 scores for logistic regression model without regularization')\n",
    "\n",
    "clf = LogisticRegression(penalty='none', max_iter=1000).fit(V_train, y_train)\n",
    "y_train_predict = clf.predict(V_train)\n",
    "f1_train = f1_score(y_train, y_train_predict)\n",
    "print('\\tTraining data:', f1_train)\n",
    "\n",
    "y_dev_predict = clf.predict(V_dev)\n",
    "f1_dev = f1_score(y_dev, y_dev_predict)\n",
    "print('\\tDevelopment data:', f1_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3faa55ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores for logistic regression model with L1 regularization\n",
      "\tTraining data: 0.803772716816195\n",
      "\tDevelopment data: 0.7258426966292134\n"
     ]
    }
   ],
   "source": [
    "# with L1 regularization\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('F1 scores for logistic regression model with L1 regularization')\n",
    "\n",
    "clf_l1 = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000).fit(V_train, y_train)\n",
    "y_train_predict = clf_l1.predict(V_train)\n",
    "f1_train = f1_score(y_train, y_train_predict)\n",
    "print('\\tTraining data:', f1_train)\n",
    "\n",
    "y_dev_predict = clf_l1.predict(V_dev)\n",
    "f1_dev = f1_score(y_dev, y_dev_predict)\n",
    "print('\\tDevelopment data:', f1_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "25343cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores for logistic regression model with L2 regularization\n",
      "\tTraining data: 0.8083447959651535\n",
      "\tDevelopment data: 0.7278797996661102\n"
     ]
    }
   ],
   "source": [
    "# with L2 regularization\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print('F1 scores for logistic regression model with L2 regularization')\n",
    "\n",
    "\n",
    "clf_l2 = LogisticRegression(penalty='l2', solver='liblinear', max_iter=1000).fit(V_train, y_train)\n",
    "y_train_predict = clf_l2.predict(V_train)\n",
    "f1_train = f1_score(y_train, y_train_predict)\n",
    "print('\\tTraining data:', f1_train)\n",
    "\n",
    "y_dev_predict = clf_l2.predict(V_dev)\n",
    "f1_dev = f1_score(y_dev, y_dev_predict)\n",
    "print('\\tDevelopment data:',f1_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ed30bbfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trouble', 3.6615610927685114),\n",
       " ('put', 3.5353164361630283),\n",
       " ('turkey', 3.31960013331611),\n",
       " ('internet', 3.2198444246925946),\n",
       " ('hail', 3.099384491219853),\n",
       " ('thunder', 2.396460528209275),\n",
       " ('nuclear', 2.2176845744109266),\n",
       " ('course', 2.1500767370172267),\n",
       " ('investigators', 2.1336672608787985),\n",
       " ('share', 2.0998036234354207),\n",
       " ('maybe', 2.0884835411585247),\n",
       " ('free', 2.06413708477806),\n",
       " ('hurricane', 2.0606821927699377),\n",
       " ('governor', 2.0466256217678334),\n",
       " ('land', 1.9961139882897678),\n",
       " ('war', 1.9667972813048158),\n",
       " ('after', 1.9416398620573951),\n",
       " ('desolate', 1.9216269864665803),\n",
       " ('virgin', 1.8852728213855834),\n",
       " ('visit', 1.82189159034019),\n",
       " ('drought', 1.7728405710687465),\n",
       " ('drowning', 1.7666504198854094),\n",
       " ('same', 1.7399547333156387),\n",
       " ('late', 1.7225434410762008),\n",
       " ('real', 1.706497212340071),\n",
       " ('explode', 1.5937148970807815),\n",
       " ('wake', 1.5670706098828107),\n",
       " ('massacre', 1.5633722956409142),\n",
       " ('damn', 1.5572455827699547),\n",
       " ('pakistani', 1.553201492319429),\n",
       " ('fully', 1.5268755388695767),\n",
       " ('state', 1.512528813777207),\n",
       " ('allow', 1.5069475356635473),\n",
       " ('drive', 1.495125694035326),\n",
       " ('stock', 1.4880150665327319),\n",
       " ('set', 1.4751833732379143),\n",
       " ('bag', 1.4416616596989147),\n",
       " ('airplane', 1.4314528953089587),\n",
       " ('sink', 1.4209295827497899),\n",
       " ('game', 1.4188352384497795),\n",
       " ('police', 1.404989252753241),\n",
       " ('pm', 1.3750829727060356),\n",
       " ('emergency', 1.3670976996768736),\n",
       " ('bleed', 1.3535232421563832),\n",
       " ('million', 1.3498501851191613),\n",
       " ('search', 1.3442018449841853),\n",
       " ('drink', 1.3405405351745234),\n",
       " ('severe', 1.3394995444153879),\n",
       " ('crush', 1.33803732165274),\n",
       " ('devastation', 1.3353395009631523),\n",
       " ('miss', 1.3041134652606095),\n",
       " ('charged', 1.281612617795174),\n",
       " ('family', 1.2803350483752616),\n",
       " ('right', 1.2750390614131555),\n",
       " ('problem', 1.2623960229563862),\n",
       " ('weapon', 1.2491552496420204),\n",
       " ('landslide', 1.2461714594250728),\n",
       " ('derailment', 1.2336911487417654),\n",
       " ('point', 1.1914440668627402),\n",
       " ('former', 1.1897723282569028),\n",
       " ('book', 1.1789279238104629),\n",
       " ('wrong', 1.1736871565108355),\n",
       " ('deal', 1.1300451934309186),\n",
       " ('earthquake', 1.1285434973185249),\n",
       " ('refugees', 1.1208988308199503),\n",
       " ('sandstorm', 1.1127497417200176),\n",
       " ('bed', 1.1042380530603872),\n",
       " ('shooting', 1.103668643919933),\n",
       " ('plan', 1.0957456724499932),\n",
       " ('helicopter', 1.090081111902306),\n",
       " ('reactor', 1.0691211009867743),\n",
       " ('act', 1.0592560386865415),\n",
       " ('failure', 1.0563330837059388),\n",
       " ('morning', 1.0515409369805848),\n",
       " ('damage', 1.0464705781711354),\n",
       " ('seismic', 1.0403666811504901),\n",
       " ('lets', 1.039530799719708),\n",
       " ('continue', 1.0335368859433978),\n",
       " ('must', 1.0223292839879847),\n",
       " ('bomb', 1.0157404390673304),\n",
       " ('okay', 0.98901028030913),\n",
       " ('government', 0.9789077902893523),\n",
       " ('country', 0.957892782259961),\n",
       " ('include', 0.9567391162642785),\n",
       " ('idea', 0.9407696826541989),\n",
       " ('die', 0.9387696709775363),\n",
       " ('fire', 0.9293446733505827),\n",
       " ('bomber', 0.9175452147483252),\n",
       " ('friend', 0.9174913913939551),\n",
       " ('mosque', 0.9081419300381904),\n",
       " ('isnt', 0.8816121204885947),\n",
       " ('couple', 0.8761882922073755),\n",
       " ('flooding', 0.8618768112723755),\n",
       " ('woman', 0.8456091330263448),\n",
       " ('fight', 0.8273545919459131),\n",
       " ('bags', 0.8257032714923944),\n",
       " ('minute', 0.8170967562549947),\n",
       " ('heart', 0.8034063898029017),\n",
       " ('mh370', 0.7904277647285569),\n",
       " ('full', 0.7882816499536448),\n",
       " ('lot', 0.7720320479845961),\n",
       " ('light', 0.7716466494166907),\n",
       " ('pakistan', 0.7645467816791276),\n",
       " ('horror', 0.7618537703680686),\n",
       " ('even', 0.7585076486406941),\n",
       " ('think', 0.7542321153096817),\n",
       " ('content', 0.737794615094742),\n",
       " ('want', 0.7333310523900685),\n",
       " ('street', 0.7255833197523093),\n",
       " ('blue', 0.7238606119819414),\n",
       " ('run', 0.7110164135618333),\n",
       " ('soul', 0.6995096384860932),\n",
       " ('always', 0.6903984330072748),\n",
       " ('snowstorm', 0.6836863909083676),\n",
       " ('oh', 0.6744957917494696),\n",
       " ('edm', 0.663287151058896),\n",
       " ('displaced', 0.6580893411920972),\n",
       " ('four', 0.6316430663349396),\n",
       " ('13', 0.6251599476428514),\n",
       " ('violent', 0.613341393196442),\n",
       " ('rain', 0.6123585254024001),\n",
       " ('beat', 0.5893015625269157),\n",
       " ('open', 0.5851202733581626),\n",
       " ('bridge', 0.5841265621115669),\n",
       " ('business', 0.5744172837602541),\n",
       " ('japan', 0.5582663968644729),\n",
       " ('fires', 0.5531416319256574),\n",
       " ('fear', 0.5526521944655378),\n",
       " ('amp', 0.5483702348573498),\n",
       " ('rioting', 0.5481476423333576),\n",
       " ('due', 0.5280825541460394),\n",
       " ('tweet', 0.5267084423476743),\n",
       " ('support', 0.5107802469109706),\n",
       " ('world', 0.510261915415783),\n",
       " ('shot', 0.5038605431390764),\n",
       " ('song', 0.49780300454019316),\n",
       " ('flood', 0.47838755105437286),\n",
       " ('riot', 0.46650799419477434),\n",
       " ('result', 0.45881028351337955),\n",
       " ('detonate', 0.45731444479124655),\n",
       " ('traumatise', 0.4543532138860717),\n",
       " ('order', 0.4486399322543249),\n",
       " ('good', 0.4429462350849212),\n",
       " ('three', 0.4424217378091148),\n",
       " ('ur', 0.4211212741936094),\n",
       " ('most', 0.41619561147804435),\n",
       " ('armageddon', 0.4064845167326515),\n",
       " ('last', 0.3981985863471189),\n",
       " ('until', 0.39793984563975643),\n",
       " ('california', 0.3922780346218153),\n",
       " ('ill', 0.3883725336542654),\n",
       " ('party', 0.38599433490569046),\n",
       " ('already', 0.37929074645918703),\n",
       " ('dance', 0.3701822819477829),\n",
       " ('call', 0.36814435990209327),\n",
       " ('atomic', 0.36671648793432277),\n",
       " ('early', 0.3651487805958518),\n",
       " ('attack', 0.35969794534925575),\n",
       " ('downtown', 0.35793303729546894),\n",
       " ('without', 0.35406406961909137),\n",
       " ('stand', 0.3513837567302963),\n",
       " ('north', 0.3410860680565256),\n",
       " ('sit', 0.33882104851685424),\n",
       " ('memory', 0.337878509921967),\n",
       " ('northern', 0.3338121732476103),\n",
       " ('spring', 0.3219379396343986),\n",
       " ('shift', 0.3195758655881964),\n",
       " ('arson', 0.31638536121694294),\n",
       " ('see', 0.31440632514873146),\n",
       " ('25', 0.3033127504184497),\n",
       " ('explosion', 0.30327427605567236),\n",
       " ('high', 0.30069589200343483),\n",
       " ('little', 0.29958997202462023),\n",
       " ('murderer', 0.28251881147309255),\n",
       " ('major', 0.2573441344510831),\n",
       " ('kill', 0.255815937222796),\n",
       " ('photo', 0.2518758829469164),\n",
       " ('charge', 0.24822045243213367),\n",
       " ('vehicle', 0.24181869154869626),\n",
       " ('curfew', 0.2405901559221767),\n",
       " ('army', 0.23446917275585494),\n",
       " ('terrorism', 0.2320148892394116),\n",
       " ('life', 0.22240449112560157),\n",
       " ('black', 0.21850251581952673),\n",
       " ('temple', 0.21482733053789096),\n",
       " ('join', 0.20102502652740473),\n",
       " ('myself', 0.19158873691429612),\n",
       " ('water', 0.18467471655084974),\n",
       " ('alarm', 0.17328113575317566),\n",
       " ('theater', 0.17097138866924214),\n",
       " ('august', 0.16933489689707237),\n",
       " ('mayhem', 0.16426471478913263),\n",
       " ('building', 0.15579347501358837),\n",
       " ('screaming', 0.1537390250448555),\n",
       " ('injure', 0.140372244523084),\n",
       " ('yours', 0.13738117532164054),\n",
       " ('crew', 0.13484745093256112),\n",
       " ('haha', 0.12981443090455722),\n",
       " ('strike', 0.1296906845210104),\n",
       " ('floods', 0.12311945224261094),\n",
       " ('feeling', 0.12203264133156744),\n",
       " ('move', 0.12137392463069206),\n",
       " ('traumatised', 0.12002759036832795),\n",
       " ('hot', 0.11558209142608325),\n",
       " ('young', 0.10978861256766202),\n",
       " ('probably', 0.10956522177668154),\n",
       " ('longer', 0.1076029462700117),\n",
       " ('omg', 0.10135325700584875),\n",
       " ('dead', 0.09231383074457676),\n",
       " ('now', 0.0916440898089937),\n",
       " ('story', 0.08832762585880471),\n",
       " ('head', 0.08608093316422702),\n",
       " ('week', 0.07728758100874353),\n",
       " ('fast', 0.07552394044061399),\n",
       " ('upon', 0.07435698327451508),\n",
       " ('line', 0.07320815169148363),\n",
       " ('mass', 0.06032979917919809),\n",
       " ('quiz', 0.057584644566772515),\n",
       " ('twister', 0.05707923561277502),\n",
       " ('play', 0.05164427942340091),\n",
       " ('while', 0.05140998214333865),\n",
       " ('responder', 0.05096824602465017),\n",
       " ('block', 0.04913877248453775),\n",
       " ('plane', 0.039302979094226836),\n",
       " ('take', 0.03889436328299894),\n",
       " ('get', 0.037844407234034955),\n",
       " ('aftershock', 0.030215651715372216),\n",
       " ('sorry', 0.026243962233505135),\n",
       " ('debris', 0.024723425874197773),\n",
       " ('time', 0.01977947317741803),\n",
       " ('much', 0.019457394251963425),\n",
       " ('power', 0.004001295566250812),\n",
       " ('crazy', 0.0030889748845336746),\n",
       " ('100', 0.0),\n",
       " ('15', 0.0),\n",
       " ('40', 0.0),\n",
       " ('50', 0.0),\n",
       " ('70', 0.0),\n",
       " ('absolutely', 0.0),\n",
       " ('accident', 0.0),\n",
       " ('action', 0.0),\n",
       " ('ago', 0.0),\n",
       " ('airport', 0.0),\n",
       " ('album', 0.0),\n",
       " ('almost', 0.0),\n",
       " ('am', 0.0),\n",
       " ('american', 0.0),\n",
       " ('amid', 0.0),\n",
       " ('another', 0.0),\n",
       " ('anthrax', 0.0),\n",
       " ('apocalypse', 0.0),\n",
       " ('appear', 0.0),\n",
       " ('area', 0.0),\n",
       " ('arent', 0.0),\n",
       " ('aug', 0.0),\n",
       " ('back', 0.0),\n",
       " ('bad', 0.0),\n",
       " ('ban', 0.0),\n",
       " ('bar', 0.0),\n",
       " ('battle', 0.0),\n",
       " ('bbc', 0.0),\n",
       " ('beach', 0.0),\n",
       " ('become', 0.0),\n",
       " ('best', 0.0),\n",
       " ('bioterrorism', 0.0),\n",
       " ('blaze', 0.0),\n",
       " ('bloody', 0.0),\n",
       " ('blow', 0.0),\n",
       " ('board', 0.0),\n",
       " ('bombing', 0.0),\n",
       " ('break', 0.0),\n",
       " ('bring', 0.0),\n",
       " ('british', 0.0),\n",
       " ('buy', 0.0),\n",
       " ('cameroon', 0.0),\n",
       " ('care', 0.0),\n",
       " ('career', 0.0),\n",
       " ('casualties', 0.0),\n",
       " ('casualty', 0.0),\n",
       " ('catastrophe', 0.0),\n",
       " ('catastrophic', 0.0),\n",
       " ('catch', 0.0),\n",
       " ('central', 0.0),\n",
       " ('change', 0.0),\n",
       " ('chemical', 0.0),\n",
       " ('city', 0.0),\n",
       " ('comment', 0.0),\n",
       " ('county', 0.0),\n",
       " ('crisis', 0.0),\n",
       " ('cut', 0.0),\n",
       " ('cyclone', 0.0),\n",
       " ('day', 0.0),\n",
       " ('death', 0.0),\n",
       " ('demolition', 0.0),\n",
       " ('disaster', 0.0),\n",
       " ('during', 0.0),\n",
       " ('east', 0.0),\n",
       " ('ebay', 0.0),\n",
       " ('enjoy', 0.0),\n",
       " ('enough', 0.0),\n",
       " ('entire', 0.0),\n",
       " ('evacuate', 0.0),\n",
       " ('evacuation', 0.0),\n",
       " ('event', 0.0),\n",
       " ('everyone', 0.0),\n",
       " ('everything', 0.0),\n",
       " ('eye', 0.0),\n",
       " ('face', 0.0),\n",
       " ('fall', 0.0),\n",
       " ('fan', 0.0),\n",
       " ('fatal', 0.0),\n",
       " ('fedex', 0.0),\n",
       " ('feel', 0.0),\n",
       " ('first', 0.0),\n",
       " ('flame', 0.0),\n",
       " ('flash', 0.0),\n",
       " ('flatten', 0.0),\n",
       " ('france', 0.0),\n",
       " ('front', 0.0),\n",
       " ('fun', 0.0),\n",
       " ('girl', 0.0),\n",
       " ('give', 0.0),\n",
       " ('glass', 0.0),\n",
       " ('global', 0.0),\n",
       " ('govt', 0.0),\n",
       " ('great', 0.0),\n",
       " ('ground', 0.0),\n",
       " ('guess', 0.0),\n",
       " ('had', 0.0),\n",
       " ('happen', 0.0),\n",
       " ('happy', 0.0),\n",
       " ('harm', 0.0),\n",
       " ('hat', 0.0),\n",
       " ('hazardous', 0.0),\n",
       " ('health', 0.0),\n",
       " ('hear', 0.0),\n",
       " ('heat', 0.0),\n",
       " ('heavy', 0.0),\n",
       " ('help', 0.0),\n",
       " ('here', 0.0),\n",
       " ('hey', 0.0),\n",
       " ('hijack', 0.0),\n",
       " ('hiroshima', 0.0),\n",
       " ('hit', 0.0),\n",
       " ('home', 0.0),\n",
       " ('hostage', 0.0),\n",
       " ('hour', 0.0),\n",
       " ('india', 0.0),\n",
       " ('injured', 0.0),\n",
       " ('involve', 0.0),\n",
       " ('israeli', 0.0),\n",
       " ('issue', 0.0),\n",
       " ('ive', 0.0),\n",
       " ('job', 0.0),\n",
       " ('jobs', 0.0),\n",
       " ('lab', 0.0),\n",
       " ('lava', 0.0),\n",
       " ('led', 0.0),\n",
       " ('let', 0.0),\n",
       " ('lie', 0.0),\n",
       " ('likely', 0.0),\n",
       " ('lmao', 0.0),\n",
       " ('lol', 0.0),\n",
       " ('look', 0.0),\n",
       " ('lord', 0.0),\n",
       " ('lucky', 0.0),\n",
       " ('mad', 0.0),\n",
       " ('make', 0.0),\n",
       " ('mark', 0.0),\n",
       " ('may', 0.0),\n",
       " ('meltdown', 0.0),\n",
       " ('men', 0.0),\n",
       " ('migrant', 0.0),\n",
       " ('mode', 0.0),\n",
       " ('money', 0.0),\n",
       " ('month', 0.0),\n",
       " ('near', 0.0),\n",
       " ('nearly', 0.0),\n",
       " ('new', 0.0),\n",
       " ('news', 0.0),\n",
       " ('nice', 0.0),\n",
       " ('nigga', 0.0),\n",
       " ('nws', 0.0),\n",
       " ('obliteration', 0.0),\n",
       " ('occur', 0.0),\n",
       " ('online', 0.0),\n",
       " ('out', 0.0),\n",
       " ('outrage', 0.0),\n",
       " ('outside', 0.0),\n",
       " ('pandemonium', 0.0),\n",
       " ('panic', 0.0),\n",
       " ('part', 0.0),\n",
       " ('patience', 0.0),\n",
       " ('people', 0.0),\n",
       " ('person', 0.0),\n",
       " ('policy', 0.0),\n",
       " ('poor', 0.0),\n",
       " ('press', 0.0),\n",
       " ('public', 0.0),\n",
       " ('question', 0.0),\n",
       " ('radiation', 0.0),\n",
       " ('radio', 0.0),\n",
       " ('rainstorm', 0.0),\n",
       " ('rd', 0.0),\n",
       " ('refugee', 0.0),\n",
       " ('refugio', 0.0),\n",
       " ('rescue', 0.0),\n",
       " ('return', 0.0),\n",
       " ('road', 0.0),\n",
       " ('russia', 0.0),\n",
       " ('saipan', 0.0),\n",
       " ('saw', 0.0),\n",
       " ('scene', 0.0),\n",
       " ('scream', 0.0),\n",
       " ('screams', 0.0),\n",
       " ('self', 0.0),\n",
       " ('sensorsenso', 0.0),\n",
       " ('serious', 0.0),\n",
       " ('service', 0.0),\n",
       " ('sick', 0.0),\n",
       " ('sign', 0.0),\n",
       " ('sinkhole', 0.0),\n",
       " ('sirens', 0.0),\n",
       " ('site', 0.0),\n",
       " ('smoke', 0.0),\n",
       " ('soon', 0.0),\n",
       " ('sound', 0.0),\n",
       " ('spill', 0.0),\n",
       " ('st', 0.0),\n",
       " ('still', 0.0),\n",
       " ('such', 0.0),\n",
       " ('suicide', 0.0),\n",
       " ('summer', 0.0),\n",
       " ('sure', 0.0),\n",
       " ('suspect', 0.0),\n",
       " ('test', 0.0),\n",
       " ('thanks', 0.0),\n",
       " ('theres', 0.0),\n",
       " ('these', 0.0),\n",
       " ('thing', 0.0),\n",
       " ('those', 0.0),\n",
       " ('thunderstorm', 0.0),\n",
       " ('tornado', 0.0),\n",
       " ('totally', 0.0),\n",
       " ('traffic', 0.0),\n",
       " ('tragedy', 0.0),\n",
       " ('train', 0.0),\n",
       " ('trauma', 0.0),\n",
       " ('tree', 0.0),\n",
       " ('truck', 0.0),\n",
       " ('turkish', 0.0),\n",
       " ('turn', 0.0),\n",
       " ('two', 0.0),\n",
       " ('under', 0.0),\n",
       " ('united', 0.0),\n",
       " ('update', 0.0),\n",
       " ('upheaval', 0.0),\n",
       " ('use', 0.0),\n",
       " ('victim', 0.0),\n",
       " ('video', 0.0),\n",
       " ('warning', 0.0),\n",
       " ('watch', 0.0),\n",
       " ('wave', 0.0),\n",
       " ('weather', 0.0),\n",
       " ('whats', 0.0),\n",
       " ('whirlwind', 0.0),\n",
       " ('wild', 0.0),\n",
       " ('wonder', 0.0),\n",
       " ('wont', 0.0),\n",
       " ('wound', 0.0),\n",
       " ('yeah', 0.0),\n",
       " ('yet', 0.0),\n",
       " ('youth', 0.0),\n",
       " ('zone', 0.0),\n",
       " ('ûò', 0.0),\n",
       " ('possible', -0.01278591276791401),\n",
       " ('car', -0.013773798279533416),\n",
       " ('typhoon', -0.024360933120404184),\n",
       " ('up', -0.024538461363603287),\n",
       " ('storm', -0.030924342017450937),\n",
       " ('engulfed', -0.0320614646485628),\n",
       " ('room', -0.03510375697996775),\n",
       " ('calgary', -0.036901539957567175),\n",
       " ('officials', -0.039070987323743187),\n",
       " ('come', -0.0470049596107292),\n",
       " ('forget', -0.049004715661565154),\n",
       " ('west', -0.053371670885623064),\n",
       " ('20', -0.0535679018181161),\n",
       " ('buildings', -0.06459298736328745),\n",
       " ('human', -0.06811307943968303),\n",
       " ('today', -0.07469930017294604),\n",
       " ('12', -0.07821465967176858),\n",
       " ('release', -0.08453164065440559),\n",
       " ('camp', -0.08579060106318269),\n",
       " ('cry', -0.08676609226308904),\n",
       " ('steal', -0.08850186518009973),\n",
       " ('one', -0.09180269720598255),\n",
       " ('word', -0.09239908899469809),\n",
       " ('11', -0.09595059005113192),\n",
       " ('kid', -0.09710241140573687),\n",
       " ('grow', -0.10463724141664828),\n",
       " ('collision', -0.10464450595979856),\n",
       " ('night', -0.10749385304807495),\n",
       " ('bioterror', -0.11387968807150817),\n",
       " ('famine', -0.11493312068322681),\n",
       " ('drown', -0.12887075125378955),\n",
       " ('report', -0.13109838663413212),\n",
       " ('legionnaires', -0.13327526461511538),\n",
       " ('hazard', -0.13428171297579045),\n",
       " ('ask', -0.1355798754687272),\n",
       " ('all', -0.139367462002937),\n",
       " ('pain', -0.14465028587581114),\n",
       " ('crash', -0.15725854594992333),\n",
       " ('team', -0.16156053524066638),\n",
       " ('door', -0.16732805009249865),\n",
       " ('pick', -0.16786569146070573),\n",
       " ('lose', -0.1682350048059348),\n",
       " ('class', -0.1754033258923558),\n",
       " ('loud', -0.1914107581708727),\n",
       " ('siren', -0.20041157334720025),\n",
       " ('god', -0.2022468605661241),\n",
       " ('giant', -0.20247637132668686),\n",
       " ('mudslide', -0.2056322316052982),\n",
       " ('case', -0.2157724021031451),\n",
       " ('carry', -0.2256966998399012),\n",
       " ('peace', -0.23734324940215934),\n",
       " ('leave', -0.2519240988594474),\n",
       " ('nowplaying', -0.2529467786483402),\n",
       " ('hailstorm', -0.25820845582251123),\n",
       " ('trapped', -0.2589520612575934),\n",
       " ('boat', -0.2627107552687235),\n",
       " ('phone', -0.26526333640010147),\n",
       " ('ball', -0.2670641260328675),\n",
       " ('burn', -0.2678332416725896),\n",
       " ('gop', -0.2688106227310576),\n",
       " ('confirmed', -0.2732633426011841),\n",
       " ('bus', -0.2784992939039625),\n",
       " ('lightning', -0.2881185166927995),\n",
       " ('ship', -0.29300984232094823),\n",
       " ('sunk', -0.2940099470070104),\n",
       " ('stop', -0.29839215086961013),\n",
       " ('libya', -0.30172750003114834),\n",
       " ('know', -0.31706064525063843),\n",
       " ('might', -0.3262614582963003),\n",
       " ('abc', -0.353917152664211),\n",
       " ('pay', -0.3549681755825096),\n",
       " ('horrible', -0.36347281937013076),\n",
       " ('village', -0.3639913014963377),\n",
       " ('trust', -0.38618377482900434),\n",
       " ('ave', -0.39162915671461057),\n",
       " ('muslims', -0.3958806419900713),\n",
       " ('then', -0.40915209959897947),\n",
       " ('check', -0.4173810979540409),\n",
       " ('other', -0.4233968987133321),\n",
       " ('ass', -0.4257969610192123),\n",
       " ('usa', -0.43129673789492423),\n",
       " ('spot', -0.4370021971294545),\n",
       " ('shots', -0.4373502588294761),\n",
       " ('wind', -0.4476081188333913),\n",
       " ('york', -0.44884894257730584),\n",
       " ('live', -0.44927710726611403),\n",
       " ('emmerdale', -0.4568645023721632),\n",
       " ('australia', -0.4631904643923005),\n",
       " ('long', -0.46386786096040095),\n",
       " ('group', -0.4698396122512352),\n",
       " ('outbreak', -0.48159385754450923),\n",
       " ('wildfire', -0.4908229036034003),\n",
       " ('fucking', -0.49567584320611163),\n",
       " ('center', -0.5015549253914462),\n",
       " ('least', -0.5027608811469113),\n",
       " ('every', -0.5044963801602503),\n",
       " ('show', -0.5369173449171504),\n",
       " ('terror', -0.5403961803511449),\n",
       " ('fukushima', -0.5405578129327431),\n",
       " ('japanese', -0.542118988165762),\n",
       " ('love', -0.543865960353083),\n",
       " ('south', -0.5515427998526335),\n",
       " ('down', -0.5536570803352966),\n",
       " ('id', -0.5568894004827905),\n",
       " ('body', -0.5643071630581987),\n",
       " ('over', -0.5700610477098731),\n",
       " ('music', -0.5911578992436114),\n",
       " ('prebreak', -0.5921156337154153),\n",
       " ('find', -0.6017912715317144),\n",
       " ('thank', -0.6035702187235285),\n",
       " ('burning', -0.6058346818036359),\n",
       " ('taiwan', -0.6089432783584963),\n",
       " ('collapse', -0.6100989967581855),\n",
       " ('texas', -0.6139946330783999),\n",
       " ('ruin', -0.616051853409871),\n",
       " ('year', -0.6194504838904051),\n",
       " ('control', -0.6294239168696801),\n",
       " ('anyone', -0.6351209545683425),\n",
       " ('cnn', -0.6437301075729255),\n",
       " ('boy', -0.6462564123722232),\n",
       " ('fuck', -0.6734750022557049),\n",
       " ('why', -0.6744148558639557),\n",
       " ('bay', -0.6811726232643879),\n",
       " ('murder', -0.6852266335133493),\n",
       " ('forest', -0.6981662376431657),\n",
       " ('both', -0.7073822215351007),\n",
       " ('stretcher', -0.714785041667144),\n",
       " ('confirm', -0.7205046501678692),\n",
       " ('where', -0.7567885817222985),\n",
       " ('volcano', -0.7649178827181041),\n",
       " ('company', -0.7833142960836542),\n",
       " ('huge', -0.8266712918714901),\n",
       " ('obliterate', -0.8318774786180845),\n",
       " ('off', -0.8406213334820296),\n",
       " ('national', -0.8410152160921016),\n",
       " ('learn', -0.8524870835881344),\n",
       " ('ebola', -0.8673518730342623),\n",
       " ('annihilate', -0.8751962028607746),\n",
       " ('terrorist', -0.928319728030918),\n",
       " ('close', -0.9364760135914385),\n",
       " ('cake', -0.9369070536669655),\n",
       " ('need', -0.9481572309074466),\n",
       " ('survive', -0.9539803320865895),\n",
       " ('demolish', -0.9664149310351131),\n",
       " ('myanmar', -0.9791587511722822),\n",
       " ('survivor', -0.9807106638081236),\n",
       " ('blizzard', -0.9847541867157286),\n",
       " ('stay', -1.0140267704239925),\n",
       " ('since', -1.0192309554910763),\n",
       " ('cover', -1.0550794961442225),\n",
       " ('swallowed', -1.0972460636791788),\n",
       " ('derail', -1.1135692185754509),\n",
       " ('start', -1.193934262136879),\n",
       " ('israel', -1.2105275818919579),\n",
       " ('blazing', -1.2138466977294071),\n",
       " ('past', -1.2407037256224271),\n",
       " ('warn', -1.4093238995536859),\n",
       " ('iran', -1.5381429320091287),\n",
       " ('work', -1.5435881762769854),\n",
       " ('blight', -1.571716884516432),\n",
       " ('top', -1.6373558048469747),\n",
       " ('should', -1.6778959917075984),\n",
       " ('typhoondevastated', -2.2229386777643416)]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dict = dict()\n",
    "words = vtz.inverse_transform(clf_l1.coef_)[0]\n",
    "for i in range(len(words)):\n",
    "    param_dict[words[i]] = clf_l1.coef_[0][i]\n",
    "\n",
    "sorted_dict = sorted(param_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e53bad",
   "metadata": {},
   "source": [
    "## Part f: Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2e2a7fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7859019264448336\n"
     ]
    }
   ],
   "source": [
    "# Bernoulli Naive Bayes\n",
    "\n",
    "def nb_predictions(x, psis, phis, K):\n",
    "    # adjust shapes\n",
    "    n, d = x.shape\n",
    "    x = np.reshape(x, (1, n, d))\n",
    "    psis = np.reshape(psis, (K, 1, d))\n",
    "    \n",
    "    # clip probabilities to avoid log(0)\n",
    "    psis = psis.clip(1e-14, 1-1e-14)\n",
    "    \n",
    "    # compute log-probabilities\n",
    "    logpy = np.log(phis).reshape([K,1])\n",
    "    logpxy = x * np.log(psis) + (1-x) * np.log(1-psis)\n",
    "    logpyx = logpxy.sum(axis=2) + logpy\n",
    "\n",
    "    return logpyx.argmax(axis=0).flatten(), logpyx.reshape([K,n])\n",
    "\n",
    "\n",
    "def Bernoulli_Naive_Bayes(xtrain, ytrain, xdev, K, alpha):\n",
    "    n = xtrain.shape[0]  # number of tweets\n",
    "    d = xtrain.shape[1]  # number of words in dataset\n",
    "    psis = np.zeros([K,d])\n",
    "    phis = np.zeros([K])\n",
    "\n",
    "    for k in range(K):\n",
    "        X_k = xtrain[ytrain == k]\n",
    "        psis[k] = (np.sum(X_k, axis=0) + alpha) / (X_k.shape[0] + 2 * alpha)\n",
    "        phis[k] = X_k.shape[0] / float(n)  \n",
    "\n",
    "    return nb_predictions(xdev, psis, phis, K)[0]\n",
    "    \n",
    "idx = Bernoulli_Naive_Bayes(V_train, y_train, V_dev, K = 2, alpha = 1)\n",
    "print(f1_score(idx, y_dev, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb52409",
   "metadata": {},
   "source": [
    "## Part g: Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996eb4fd",
   "metadata": {},
   "source": [
    "model comparison here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f837cd7",
   "metadata": {},
   "source": [
    "## Part h: N-gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "83bcae5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 nigerian\n",
      "15 saudi\n",
      "16yr old\n",
      "2015 prebreak\n",
      "40 family\n",
      "70 year\n",
      "add video\n",
      "affect fatal\n",
      "after waving\n",
      "air ambulance\n"
     ]
    }
   ],
   "source": [
    "# N-gram model\n",
    "\n",
    "M2 = 10\n",
    "vectorizer2 = CountVectorizer(binary=True, min_df=M2, ngram_range=(1,2))\n",
    "vtz2 = vectorizer2.fit(X_train['text'])\n",
    "V_train2 = vtz2.transform(X_train['text']).toarray()\n",
    "V_dev2 = vtz2.transform(X_dev['text']).toarray()\n",
    "\n",
    "features = vectorizer2.get_feature_names_out()\n",
    "counter = 0\n",
    "for feature in features:\n",
    "    if ' ' in feature and counter < 10:\n",
    "        print(feature)\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7b63aa25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1039"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of 1-grams\n",
    "len(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7538c598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1260"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of 2-grams\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0521f364",
   "metadata": {},
   "source": [
    "Logistic regression with L2 is chosen because it has highest F-score in development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7940ae59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining data: 0.811407543698252\n",
      "\tDevelopment data: 0.7268673355629878\n"
     ]
    }
   ],
   "source": [
    "clf_2gram = LogisticRegression(penalty='l2', solver='liblinear', max_iter=1000).fit(V_train2, y_train)\n",
    "y_train_predict = clf_2gram.predict(V_train2)\n",
    "f1_train = f1_score(y_train, y_train_predict)\n",
    "print('\\tTraining data:', f1_train)\n",
    "\n",
    "y_dev_predict = clf_2gram.predict(V_dev2)\n",
    "f1_dev = f1_score(y_dev, y_dev_predict)\n",
    "print('\\tDevelopment data:',f1_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8ade116c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8018389941827735\n",
      "0.7898423817863398\n"
     ]
    }
   ],
   "source": [
    "idx_train = Bernoulli_Naive_Bayes(V_train2, y_train, V_train2, K = 2, alpha = 1)\n",
    "print(f1_score(idx_train, y_train, average='micro'))\n",
    "\n",
    "idx_dev = Bernoulli_Naive_Bayes(V_train2, y_train, V_dev2, K = 2, alpha = 1)\n",
    "print(f1_score(idx_dev, y_dev, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b10dff",
   "metadata": {},
   "source": [
    "## Part i: Determine performance with the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e7c3adb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = preprocess_text(test['text'])\n",
    "train['text'] = preprocess_text(train['text'])\n",
    "\n",
    "M3 = 10\n",
    "vectorizer3 = CountVectorizer(binary=True, min_df=M3, ngram_range=(1,2))\n",
    "vtz3 = vectorizer3.fit(train['text'])\n",
    "V_train3 = vtz3.transform(train['text']).toarray()\n",
    "V_test = vtz3.transform(test['text']).toarray()\n",
    "idx_test = Bernoulli_Naive_Bayes(V_train3, train['target'], V_test, K = 2, alpha = 1)\n",
    "\n",
    "# Generates final df that is used for creating a csv file\n",
    "final_df = pd.DataFrame({'id': test['id'], 'target': idx_test})\n",
    "final_df.to_csv('disaster_predicted.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e99648",
   "metadata": {},
   "source": [
    "![output.png](output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250cf586",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
