{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef761591",
   "metadata": {},
   "source": [
    "# Homework 2 Programming Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "c43fda68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "20e7cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "20a5a4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data points: 7613\n",
      "Test data points: 3263\n"
     ]
    }
   ],
   "source": [
    "print('Training data points:', len(train))\n",
    "print('Test data points:', len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac6c8f8",
   "metadata": {},
   "source": [
    "There are 7613 training data points and 3263 test data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "d2fefd8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4296597924602653"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[train['target'] == 1]) / len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "bb908310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5703402075397347"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[train['target'] == 0]) / len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd2156b",
   "metadata": {},
   "source": [
    "\\~43% of the training tweets are about real disasters and  \\~57% of the training tweets are not about real disasters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "5219f13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "3b1b2884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>686</td>\n",
       "      <td>attack</td>\n",
       "      <td>#UNITE THE BLUE</td>\n",
       "      <td>@blazerfan not everyone can see ignoranceshe i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4854</th>\n",
       "      <td>6913</td>\n",
       "      <td>mass%20murderer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>White people I know you worry tirelessly about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4270</th>\n",
       "      <td>6066</td>\n",
       "      <td>heat%20wave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chilli heat wave Doritos never fail!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>1441</td>\n",
       "      <td>body%20bagging</td>\n",
       "      <td>New Your</td>\n",
       "      <td>@BroseidonRex @dapurplesharpie I skimmed throu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4475</th>\n",
       "      <td>6365</td>\n",
       "      <td>hostages</td>\n",
       "      <td>cuba</td>\n",
       "      <td>#hot  C-130 specially modified to land in a st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4931</th>\n",
       "      <td>7025</td>\n",
       "      <td>mayhem</td>\n",
       "      <td>Manavadar, Gujarat</td>\n",
       "      <td>They are the real heroes... RIP Brave hearts.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3264</th>\n",
       "      <td>4689</td>\n",
       "      <td>engulfed</td>\n",
       "      <td>USA</td>\n",
       "      <td>Car engulfed in flames backs up traffic at Par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>2388</td>\n",
       "      <td>collapsed</td>\n",
       "      <td>Alexandria, Egypt.</td>\n",
       "      <td>Great British Bake Off's back and Dorret's cho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2607</th>\n",
       "      <td>3742</td>\n",
       "      <td>destroyed</td>\n",
       "      <td>USA</td>\n",
       "      <td>Black Eye 9: A space battle occurred at Star O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>3924</td>\n",
       "      <td>devastated</td>\n",
       "      <td>Dorset, UK</td>\n",
       "      <td>???????????? @MikeParrActor absolutely devasta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5329 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id          keyword            location  \\\n",
       "476    686           attack   #UNITE THE BLUE     \n",
       "4854  6913  mass%20murderer                 NaN   \n",
       "4270  6066      heat%20wave                 NaN   \n",
       "992   1441   body%20bagging            New Your   \n",
       "4475  6365         hostages                cuba   \n",
       "...    ...              ...                 ...   \n",
       "4931  7025           mayhem  Manavadar, Gujarat   \n",
       "3264  4689         engulfed                 USA   \n",
       "1653  2388        collapsed  Alexandria, Egypt.   \n",
       "2607  3742        destroyed                 USA   \n",
       "2732  3924       devastated          Dorset, UK   \n",
       "\n",
       "                                                   text  \n",
       "476   @blazerfan not everyone can see ignoranceshe i...  \n",
       "4854  White people I know you worry tirelessly about...  \n",
       "4270               Chilli heat wave Doritos never fail!  \n",
       "992   @BroseidonRex @dapurplesharpie I skimmed throu...  \n",
       "4475  #hot  C-130 specially modified to land in a st...  \n",
       "...                                                 ...  \n",
       "4931  They are the real heroes... RIP Brave hearts.....  \n",
       "3264  Car engulfed in flames backs up traffic at Par...  \n",
       "1653  Great British Bake Off's back and Dorret's cho...  \n",
       "2607  Black Eye 9: A space battle occurred at Star O...  \n",
       "2732  ???????????? @MikeParrActor absolutely devasta...  \n",
       "\n",
       "[5329 rows x 4 columns]"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X = list(train.columns[:-1])\n",
    "\n",
    "# Splits train.csv into training set (70%) and development set (30%)\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(train[X], train['target'], test_size=0.3, random_state=0)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "fef4e9f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "476     @blazerfan not everyone can see ignoranceshe b...\n",
       "4854    White people I know you worry tirelessly about...\n",
       "4270                 Chilli heat wave Doritos never fail!\n",
       "992     @BroseidonRex @dapurplesharpie I skim through ...\n",
       "4475    #hot C-130 specially modify to land in a stadi...\n",
       "                              ...                        \n",
       "4931    They be the real heroes... RIP Brave hearts......\n",
       "3264    Car engulfed in flame back up traffic at Parle...\n",
       "1653    Great British Bake Off's back and Dorret's cho...\n",
       "2607    Black Eye 9: A space battle occur at Star O784...\n",
       "2732    ???????????? @MikeParrActor absolutely devasta...\n",
       "Name: text, Length: 5329, dtype: object"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet \n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# import nltk\n",
    "# nltk.download()\n",
    "\n",
    "# Lemmatize words based on part of speech (verbs, adjectives, and nouns)\n",
    "def lemmatize(text):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    word_tags = pos_tag(text.split())\n",
    "    result_text = []\n",
    "    for word_tag in word_tags:\n",
    "        lemmatized_word = word_tag[0]\n",
    "        # lemmatize verbs (e.g. ate -> eat)\n",
    "        if 'VB' in word_tag[1]:\n",
    "            lemmatized_word = wnl.lemmatize(word_tag[0], pos='v')\n",
    "        # lemmatize adjectives (e.g. better -> good)\n",
    "        elif 'JJ' in word_tag[1]:\n",
    "            lemmatized_word = wnl.lemmatize(word_tag[0], pos='a')\n",
    "        # lemmatize nouns (e.g. cookies -> cookie)\n",
    "        elif 'NN' in word_tag[1]:\n",
    "            lemmatized_word = wnl.lemmatize(word_tag[0], pos='n')\n",
    "        result_text.append(lemmatized_word)\n",
    "    return ' '.join(result_text)\n",
    "\n",
    "X_train['text'] = X_train['text'].apply(lambda text: lemmatize(text))\n",
    "X_dev['text'] = X_dev['text'].apply(lambda text: lemmatize(text))\n",
    "test['text'] = test['text'].apply(lambda text: lemmatize(text))\n",
    "X_train['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "92145aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "stop_words = ['the', 'a', 'an', 'and', 'or', 'this', 'that', 'i', 'my', 'me', 'we', 'us', 'our', 'she', 'her', \n",
    "              'he', 'his', 'him', 'they', 'their', 'them', 'you', 'your', 'there', 'are', 'is', 'from', 'to',\n",
    "              'will', 'can', 'cant', 'would', 'has', 'have', 'could', 'be', 'as', 'if', 'in', 'on', 'also', 'at', \n",
    "              'of', 'into', 'by', 'be', 'it', 'its', 'so', 'im', 'youre', 'theyre', 'hes', 'shes', 'were', 'was', \n",
    "              'not','but', 'no', 'never', 'with', 'really', 'do', 'for', 'about', 'what', 'how', 'who', 'just',\n",
    "              'when', 'via', 'which', 'than', 'like']\n",
    "\n",
    "\n",
    "def regex_stop_word(words):\n",
    "    regex = r'\\b'\n",
    "    for i in range(len(words)):\n",
    "        if i == len(words) - 1:\n",
    "            regex += words[i] + r'\\b'\n",
    "        else:\n",
    "            regex += words[i] + r'\\b|\\b'\n",
    "    return regex\n",
    "\n",
    "\n",
    "def preprocess_text(X):\n",
    "    # Converts all the words to lowercase\n",
    "    X = X.apply(lambda text: text.lower())\n",
    "    \n",
    "    # Removes URLs\n",
    "    X = X.apply(lambda text: re.sub(r'http\\S+', '', text))\n",
    "    \n",
    "    # Removes user id\n",
    "    X = X.apply(lambda text: re.sub(r'@(.*?)[\\s]', ' ', text))\n",
    "    \n",
    "    # Strips punctuations\n",
    "    X = X.apply(lambda text: re.sub(r'[^\\w\\s]', '', text))\n",
    "    \n",
    "    # Strips stop words\n",
    "    X = X.apply(lambda text: re.sub(regex_stop_word(stop_words), '', text))\n",
    "    return X\n",
    "\n",
    "X_train['text'] = preprocess_text(X_train['text'])\n",
    "X_dev['text'] = preprocess_text(X_dev['text'])\n",
    "# preprocess_text(X_dev['text'])\n",
    "\n",
    "\n",
    "# # Converts all the words to lowercase\n",
    "# X_train['text'] = X_train['text'].apply(lambda text: text.lower())\n",
    "# X_dev['text'] = X_dev['text'].apply(lambda text: text.lower())\n",
    "# test['text'] = test['text'].apply(lambda text: text.lower())\n",
    "\n",
    "# # Removes URLs\n",
    "# X_train['text'] = X_train['text'].apply(lambda text: re.sub(r'http\\S+', '', text))\n",
    "# X_dev['text'] = X_dev['text'].apply(lambda text: re.sub(r'http\\S+', '', text))\n",
    "# test['text'] = test['text'].apply(lambda text: re.sub(r'http\\S+', '', text))\n",
    "\n",
    "# # Removes user id\n",
    "# X_train['text'] = X_train['text'].apply(lambda text: re.sub(r'@(.*?)[\\s]', ' ', text))\n",
    "# X_dev['text'] = X_dev['text'].apply(lambda text: re.sub(r'@(.*?)[\\s]', ' ', text))\n",
    "# test['text'] = test['text'].apply(lambda text: re.sub(r'@(.*?)[\\s]', ' ', text))\n",
    "\n",
    "# # Strips punctuations\n",
    "# X_train['text'] = X_train['text'].apply(lambda text: re.sub(r'[^\\w\\s]', '', text))\n",
    "# X_dev['text'] = X_dev['text'].apply(lambda text: re.sub(r'[^\\w\\s]', '', text))\n",
    "# test['text'] = test['text'].apply(lambda text: re.sub(r'[^\\w\\s]', '', text))\n",
    "\n",
    "# # Strips the stop words (the, a, an, and, or)\n",
    "# X_train['text'] = X_train['text'].apply(lambda text: re.sub(regex_stop_word(stop_words), '', text))\n",
    "# X_dev['text'] = X_dev['text'].apply(lambda text: re.sub(regex_stop_word(stop_words), '', text))\n",
    "# test['text'] = test['text'].apply(lambda text: re.sub(regex_stop_word(stop_words), '', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "ce8462e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "M = 10\n",
    "vectorizer = CountVectorizer(binary=True, min_df=M)\n",
    "vtz = vectorizer.fit(X_train['text'])\n",
    "V_train = vtz.transform(X_train['text']).toarray()\n",
    "V_dev = vtz.transform(X_dev['text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "eee88cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores for logistic regression model without regularization\n",
      "\tTraining data: 0.8345965225144895\n",
      "\tDevelopment data: 0.7204703367183325\n"
     ]
    }
   ],
   "source": [
    "# without regularization terms\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('F1 scores for logistic regression model without regularization')\n",
    "\n",
    "clf = LogisticRegression(penalty='none', max_iter=1000).fit(V_train, y_train)\n",
    "y_train_predict = clf.predict(V_train)\n",
    "f1_train = f1_score(y_train, y_train_predict)\n",
    "print('\\tTraining data:', f1_train)\n",
    "\n",
    "y_dev_predict = clf.predict(V_dev)\n",
    "f1_dev = f1_score(y_dev, y_dev_predict)\n",
    "print('\\tDevelopment data:', f1_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "3faa55ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores for logistic regression model with L1 regularization\n",
      "\tTraining data: 0.803772716816195\n",
      "\tDevelopment data: 0.7258426966292134\n"
     ]
    }
   ],
   "source": [
    "# with L1 regularization\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('F1 scores for logistic regression model with L1 regularization')\n",
    "\n",
    "clf_l1 = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000).fit(V_train, y_train)\n",
    "y_train_predict = clf_l1.predict(V_train)\n",
    "f1_train = f1_score(y_train, y_train_predict)\n",
    "print('\\tTraining data:', f1_train)\n",
    "\n",
    "y_dev_predict = clf_l1.predict(V_dev)\n",
    "f1_dev = f1_score(y_dev, y_dev_predict)\n",
    "print('\\tDevelopment data:', f1_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "25343cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores for logistic regression model with L2 regularization\n",
      "\tTraining data: 0.8083447959651535\n",
      "\tDevelopment data: 0.7278797996661102\n"
     ]
    }
   ],
   "source": [
    "# with L2 regularization\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print('F1 scores for logistic regression model with L2 regularization')\n",
    "\n",
    "\n",
    "clf_l2 = LogisticRegression(penalty='l2', solver='liblinear', max_iter=1000).fit(V_train, y_train)\n",
    "y_train_predict = clf_l2.predict(V_train)\n",
    "f1_train = f1_score(y_train, y_train_predict)\n",
    "print('\\tTraining data:', f1_train)\n",
    "\n",
    "y_dev_predict = clf_l2.predict(V_dev)\n",
    "f1_dev = f1_score(y_dev, y_dev_predict)\n",
    "print('\\tDevelopment data:',f1_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "ed30bbfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trouble', 3.6616588013293865),\n",
       " ('put', 3.5353008012636558),\n",
       " ('turkey', 3.3195778343668287),\n",
       " ('internet', 3.2198276061751026),\n",
       " ('hail', 3.0993811583985003),\n",
       " ('thunder', 2.3964299579877095),\n",
       " ('nuclear', 2.2176398583144805),\n",
       " ('course', 2.149982341490194),\n",
       " ('investigators', 2.133618865482038),\n",
       " ('share', 2.0996074639763602),\n",
       " ('maybe', 2.0885411948041117),\n",
       " ('free', 2.0642003646094045),\n",
       " ('hurricane', 2.0607044180084455),\n",
       " ('governor', 2.046488424669268),\n",
       " ('land', 1.9960885723513488),\n",
       " ('war', 1.9669043609716055),\n",
       " ('after', 1.9416501959111787),\n",
       " ('desolate', 1.9216226728336159),\n",
       " ('virgin', 1.885879975101649),\n",
       " ('visit', 1.8223607437217386),\n",
       " ('drought', 1.7727803537765974),\n",
       " ('drowning', 1.7665321714200006),\n",
       " ('same', 1.7398941987797505),\n",
       " ('late', 1.7225825897200766),\n",
       " ('real', 1.7064091460872053),\n",
       " ('explode', 1.593716789156894),\n",
       " ('wake', 1.5671064499980307),\n",
       " ('massacre', 1.5633411244219133),\n",
       " ('damn', 1.5572423393762589),\n",
       " ('pakistani', 1.5531799967392053),\n",
       " ('fully', 1.5268695988095629),\n",
       " ('state', 1.5124872562916658),\n",
       " ('allow', 1.5067941170559922),\n",
       " ('drive', 1.4952050388456573),\n",
       " ('stock', 1.4879568472574627),\n",
       " ('set', 1.4751836410282175),\n",
       " ('bag', 1.441706148165073),\n",
       " ('airplane', 1.4312944052004057),\n",
       " ('sink', 1.4209175775125935),\n",
       " ('game', 1.4188181879513608),\n",
       " ('police', 1.4051123729754258),\n",
       " ('pm', 1.3752472068987356),\n",
       " ('emergency', 1.3671413480732009),\n",
       " ('bleed', 1.3534489528022042),\n",
       " ('million', 1.3498934198117956),\n",
       " ('search', 1.344276311210279),\n",
       " ('drink', 1.3405787619198213),\n",
       " ('severe', 1.339403878388849),\n",
       " ('crush', 1.3379941510137854),\n",
       " ('devastation', 1.334970120202471),\n",
       " ('miss', 1.3040204811237859),\n",
       " ('charged', 1.281577766355998),\n",
       " ('family', 1.2803090038312057),\n",
       " ('right', 1.2749702778127967),\n",
       " ('problem', 1.2624276777786398),\n",
       " ('weapon', 1.2491453398586123),\n",
       " ('landslide', 1.2461141442169639),\n",
       " ('derailment', 1.2336819372608647),\n",
       " ('point', 1.1914826705738921),\n",
       " ('former', 1.1896930514017512),\n",
       " ('book', 1.1787928514283146),\n",
       " ('wrong', 1.17387955594219),\n",
       " ('deal', 1.1300859313236418),\n",
       " ('earthquake', 1.1285774184142716),\n",
       " ('refugees', 1.1208771612792063),\n",
       " ('sandstorm', 1.1126957961991353),\n",
       " ('bed', 1.1040044176975785),\n",
       " ('shooting', 1.1037999495144986),\n",
       " ('plan', 1.0956052661936853),\n",
       " ('helicopter', 1.0900219348184714),\n",
       " ('reactor', 1.0693067099726892),\n",
       " ('act', 1.0591617325999367),\n",
       " ('failure', 1.0563426237182787),\n",
       " ('morning', 1.0515415621865714),\n",
       " ('damage', 1.0464824261656456),\n",
       " ('seismic', 1.0405597873751407),\n",
       " ('lets', 1.0394667154577588),\n",
       " ('continue', 1.0336497746577062),\n",
       " ('must', 1.0224308917387381),\n",
       " ('bomb', 1.0158256345821803),\n",
       " ('okay', 0.9890071683523529),\n",
       " ('government', 0.9788711362439766),\n",
       " ('country', 0.957812059976368),\n",
       " ('include', 0.9567082536334545),\n",
       " ('idea', 0.9407110821938286),\n",
       " ('die', 0.9388538283750456),\n",
       " ('fire', 0.9293495297243026),\n",
       " ('bomber', 0.9175229636651115),\n",
       " ('friend', 0.9175091849462852),\n",
       " ('mosque', 0.9082204547270816),\n",
       " ('isnt', 0.8816343081865451),\n",
       " ('couple', 0.8760347395029632),\n",
       " ('flooding', 0.8618572330741412),\n",
       " ('woman', 0.8456157989232819),\n",
       " ('fight', 0.8272025816264019),\n",
       " ('bags', 0.8255759644760727),\n",
       " ('minute', 0.8171151269042897),\n",
       " ('heart', 0.8034011482967127),\n",
       " ('mh370', 0.7904460270884726),\n",
       " ('full', 0.7879789192541934),\n",
       " ('light', 0.7715903757799853),\n",
       " ('lot', 0.770840574389778),\n",
       " ('pakistan', 0.7645341889758256),\n",
       " ('horror', 0.7618835332842433),\n",
       " ('even', 0.7584728739482047),\n",
       " ('think', 0.7543322753315046),\n",
       " ('content', 0.7378382242397171),\n",
       " ('want', 0.7332639762288188),\n",
       " ('street', 0.7255491742968997),\n",
       " ('blue', 0.7237598445184773),\n",
       " ('run', 0.7110025416701348),\n",
       " ('soul', 0.6995279716564571),\n",
       " ('always', 0.6905535377959207),\n",
       " ('snowstorm', 0.6836591940562705),\n",
       " ('oh', 0.674538265969882),\n",
       " ('edm', 0.6632425764486782),\n",
       " ('displaced', 0.6581508578074763),\n",
       " ('four', 0.631564059545789),\n",
       " ('13', 0.6250245459563166),\n",
       " ('violent', 0.6133231651830776),\n",
       " ('rain', 0.612291817180539),\n",
       " ('beat', 0.5892306162930512),\n",
       " ('open', 0.5850853139779019),\n",
       " ('bridge', 0.5841615098299404),\n",
       " ('business', 0.5742020848556665),\n",
       " ('japan', 0.5582184277882936),\n",
       " ('fires', 0.5530166949023547),\n",
       " ('fear', 0.5525544849508817),\n",
       " ('amp', 0.5483135047849833),\n",
       " ('rioting', 0.5481222058855093),\n",
       " ('due', 0.5280219684162275),\n",
       " ('tweet', 0.5266711322338687),\n",
       " ('support', 0.5108299695255832),\n",
       " ('world', 0.5102634493887651),\n",
       " ('shot', 0.5038962769767759),\n",
       " ('song', 0.49773676482923335),\n",
       " ('flood', 0.47851308832542533),\n",
       " ('riot', 0.4664960890646403),\n",
       " ('result', 0.45878973157635444),\n",
       " ('detonate', 0.45729444190829793),\n",
       " ('traumatise', 0.4542861805574477),\n",
       " ('order', 0.44863961363038585),\n",
       " ('good', 0.4429035712723676),\n",
       " ('three', 0.44191191055198975),\n",
       " ('ur', 0.42127441401296667),\n",
       " ('most', 0.4161472226665885),\n",
       " ('armageddon', 0.4065568504244848),\n",
       " ('last', 0.3982133059984008),\n",
       " ('until', 0.3980170589133437),\n",
       " ('california', 0.39219960017363176),\n",
       " ('ill', 0.3884877573279293),\n",
       " ('party', 0.38602416319718047),\n",
       " ('already', 0.3794199955911034),\n",
       " ('dance', 0.3701845390924263),\n",
       " ('call', 0.36816012246616264),\n",
       " ('atomic', 0.3667871250301112),\n",
       " ('early', 0.36515033256633006),\n",
       " ('attack', 0.3597085092787211),\n",
       " ('downtown', 0.3579951412352559),\n",
       " ('without', 0.3540863280958467),\n",
       " ('stand', 0.3513932144993277),\n",
       " ('north', 0.3410961669319944),\n",
       " ('sit', 0.3381363715531746),\n",
       " ('memory', 0.3378981786802688),\n",
       " ('northern', 0.33389165708874724),\n",
       " ('spring', 0.3219871844145237),\n",
       " ('shift', 0.31970611628631296),\n",
       " ('arson', 0.3164819679457078),\n",
       " ('see', 0.31457409209768616),\n",
       " ('25', 0.30325078971888475),\n",
       " ('explosion', 0.3031941203293448),\n",
       " ('high', 0.3006488362980589),\n",
       " ('little', 0.2994483597543243),\n",
       " ('murderer', 0.2824448464277103),\n",
       " ('major', 0.25736866083796295),\n",
       " ('kill', 0.25577915956401415),\n",
       " ('photo', 0.2519063103207045),\n",
       " ('charge', 0.250076920118843),\n",
       " ('vehicle', 0.2418840413260989),\n",
       " ('curfew', 0.24056171850225877),\n",
       " ('army', 0.2345877967473046),\n",
       " ('terrorism', 0.23181493964892805),\n",
       " ('life', 0.2222893478996307),\n",
       " ('black', 0.21845818631410588),\n",
       " ('temple', 0.21488896304056943),\n",
       " ('join', 0.20081498602559705),\n",
       " ('myself', 0.19145685025819234),\n",
       " ('water', 0.1846424141272216),\n",
       " ('alarm', 0.17331102726883102),\n",
       " ('theater', 0.17095471004391957),\n",
       " ('august', 0.16943325667961243),\n",
       " ('mayhem', 0.1642695290469244),\n",
       " ('building', 0.15568985954136763),\n",
       " ('screaming', 0.15375897900596355),\n",
       " ('injure', 0.14032155926693488),\n",
       " ('yours', 0.1374776427257164),\n",
       " ('crew', 0.13479796803176974),\n",
       " ('strike', 0.1298186574458644),\n",
       " ('haha', 0.12977296240345884),\n",
       " ('floods', 0.12302055069449007),\n",
       " ('feeling', 0.12203693289891761),\n",
       " ('move', 0.12137032869273419),\n",
       " ('traumatised', 0.12008915207024758),\n",
       " ('hot', 0.1156583839076212),\n",
       " ('young', 0.1098827640981891),\n",
       " ('probably', 0.10971311297668866),\n",
       " ('longer', 0.10763048062875798),\n",
       " ('omg', 0.10121251363623479),\n",
       " ('dead', 0.09227183033558273),\n",
       " ('now', 0.09162095328630945),\n",
       " ('story', 0.08827667974076386),\n",
       " ('head', 0.0860611128702496),\n",
       " ('week', 0.07723854686338745),\n",
       " ('fast', 0.07553931970579103),\n",
       " ('upon', 0.07434039748037771),\n",
       " ('line', 0.0730762017968121),\n",
       " ('mass', 0.06029039581862172),\n",
       " ('quiz', 0.0576593750110629),\n",
       " ('twister', 0.05713546868652035),\n",
       " ('play', 0.05171073050976372),\n",
       " ('while', 0.0515469396073654),\n",
       " ('responder', 0.05117200676421245),\n",
       " ('block', 0.049055997347411014),\n",
       " ('plane', 0.03938589499370921),\n",
       " ('take', 0.038856910234277106),\n",
       " ('get', 0.037927928936581354),\n",
       " ('aftershock', 0.030206941587525737),\n",
       " ('sorry', 0.026170134912367467),\n",
       " ('debris', 0.024727520746386046),\n",
       " ('time', 0.019867966022372334),\n",
       " ('much', 0.019525728293238086),\n",
       " ('power', 0.004047104849340193),\n",
       " ('crazy', 0.00308134778272047),\n",
       " ('100', 0.0),\n",
       " ('15', 0.0),\n",
       " ('40', 0.0),\n",
       " ('50', 0.0),\n",
       " ('70', 0.0),\n",
       " ('absolutely', 0.0),\n",
       " ('accident', 0.0),\n",
       " ('action', 0.0),\n",
       " ('ago', 0.0),\n",
       " ('airport', 0.0),\n",
       " ('album', 0.0),\n",
       " ('almost', 0.0),\n",
       " ('am', 0.0),\n",
       " ('american', 0.0),\n",
       " ('amid', 0.0),\n",
       " ('another', 0.0),\n",
       " ('anthrax', 0.0),\n",
       " ('apocalypse', 0.0),\n",
       " ('appear', 0.0),\n",
       " ('area', 0.0),\n",
       " ('arent', 0.0),\n",
       " ('aug', 0.0),\n",
       " ('back', 0.0),\n",
       " ('bad', 0.0),\n",
       " ('ban', 0.0),\n",
       " ('bar', 0.0),\n",
       " ('battle', 0.0),\n",
       " ('bbc', 0.0),\n",
       " ('beach', 0.0),\n",
       " ('become', 0.0),\n",
       " ('best', 0.0),\n",
       " ('bioterrorism', 0.0),\n",
       " ('blaze', 0.0),\n",
       " ('bloody', 0.0),\n",
       " ('blow', 0.0),\n",
       " ('board', 0.0),\n",
       " ('bombing', 0.0),\n",
       " ('break', 0.0),\n",
       " ('bring', 0.0),\n",
       " ('british', 0.0),\n",
       " ('buy', 0.0),\n",
       " ('cameroon', 0.0),\n",
       " ('care', 0.0),\n",
       " ('career', 0.0),\n",
       " ('casualties', 0.0),\n",
       " ('casualty', 0.0),\n",
       " ('catastrophe', 0.0),\n",
       " ('catastrophic', 0.0),\n",
       " ('catch', 0.0),\n",
       " ('central', 0.0),\n",
       " ('change', 0.0),\n",
       " ('chemical', 0.0),\n",
       " ('city', 0.0),\n",
       " ('comment', 0.0),\n",
       " ('county', 0.0),\n",
       " ('crisis', 0.0),\n",
       " ('cut', 0.0),\n",
       " ('cyclone', 0.0),\n",
       " ('day', 0.0),\n",
       " ('death', 0.0),\n",
       " ('demolition', 0.0),\n",
       " ('disaster', 0.0),\n",
       " ('during', 0.0),\n",
       " ('east', 0.0),\n",
       " ('ebay', 0.0),\n",
       " ('enjoy', 0.0),\n",
       " ('enough', 0.0),\n",
       " ('entire', 0.0),\n",
       " ('evacuate', 0.0),\n",
       " ('evacuation', 0.0),\n",
       " ('event', 0.0),\n",
       " ('everyone', 0.0),\n",
       " ('everything', 0.0),\n",
       " ('eye', 0.0),\n",
       " ('face', 0.0),\n",
       " ('fall', 0.0),\n",
       " ('fan', 0.0),\n",
       " ('fatal', 0.0),\n",
       " ('fedex', 0.0),\n",
       " ('feel', 0.0),\n",
       " ('first', 0.0),\n",
       " ('flame', 0.0),\n",
       " ('flash', 0.0),\n",
       " ('flatten', 0.0),\n",
       " ('france', 0.0),\n",
       " ('front', 0.0),\n",
       " ('fun', 0.0),\n",
       " ('girl', 0.0),\n",
       " ('give', 0.0),\n",
       " ('glass', 0.0),\n",
       " ('global', 0.0),\n",
       " ('govt', 0.0),\n",
       " ('great', 0.0),\n",
       " ('ground', 0.0),\n",
       " ('guess', 0.0),\n",
       " ('had', 0.0),\n",
       " ('happen', 0.0),\n",
       " ('happy', 0.0),\n",
       " ('harm', 0.0),\n",
       " ('hat', 0.0),\n",
       " ('hazardous', 0.0),\n",
       " ('health', 0.0),\n",
       " ('hear', 0.0),\n",
       " ('heat', 0.0),\n",
       " ('heavy', 0.0),\n",
       " ('help', 0.0),\n",
       " ('here', 0.0),\n",
       " ('hey', 0.0),\n",
       " ('hijack', 0.0),\n",
       " ('hiroshima', 0.0),\n",
       " ('hit', 0.0),\n",
       " ('home', 0.0),\n",
       " ('hostage', 0.0),\n",
       " ('hour', 0.0),\n",
       " ('india', 0.0),\n",
       " ('injured', 0.0),\n",
       " ('involve', 0.0),\n",
       " ('israeli', 0.0),\n",
       " ('issue', 0.0),\n",
       " ('ive', 0.0),\n",
       " ('job', 0.0),\n",
       " ('jobs', 0.0),\n",
       " ('lab', 0.0),\n",
       " ('lava', 0.0),\n",
       " ('led', 0.0),\n",
       " ('let', 0.0),\n",
       " ('lie', 0.0),\n",
       " ('likely', 0.0),\n",
       " ('lmao', 0.0),\n",
       " ('lol', 0.0),\n",
       " ('look', 0.0),\n",
       " ('lord', 0.0),\n",
       " ('lucky', 0.0),\n",
       " ('mad', 0.0),\n",
       " ('make', 0.0),\n",
       " ('mark', 0.0),\n",
       " ('may', 0.0),\n",
       " ('meltdown', 0.0),\n",
       " ('men', 0.0),\n",
       " ('migrant', 0.0),\n",
       " ('mode', 0.0),\n",
       " ('money', 0.0),\n",
       " ('month', 0.0),\n",
       " ('near', 0.0),\n",
       " ('nearly', 0.0),\n",
       " ('new', 0.0),\n",
       " ('news', 0.0),\n",
       " ('nice', 0.0),\n",
       " ('nigga', 0.0),\n",
       " ('nws', 0.0),\n",
       " ('obliteration', 0.0),\n",
       " ('occur', 0.0),\n",
       " ('online', 0.0),\n",
       " ('out', 0.0),\n",
       " ('outrage', 0.0),\n",
       " ('outside', 0.0),\n",
       " ('pandemonium', 0.0),\n",
       " ('panic', 0.0),\n",
       " ('part', 0.0),\n",
       " ('patience', 0.0),\n",
       " ('people', 0.0),\n",
       " ('person', 0.0),\n",
       " ('policy', 0.0),\n",
       " ('poor', 0.0),\n",
       " ('press', 0.0),\n",
       " ('public', 0.0),\n",
       " ('question', 0.0),\n",
       " ('radiation', 0.0),\n",
       " ('radio', 0.0),\n",
       " ('rainstorm', 0.0),\n",
       " ('rd', 0.0),\n",
       " ('refugee', 0.0),\n",
       " ('refugio', 0.0),\n",
       " ('rescue', 0.0),\n",
       " ('return', 0.0),\n",
       " ('road', 0.0),\n",
       " ('russia', 0.0),\n",
       " ('saipan', 0.0),\n",
       " ('saw', 0.0),\n",
       " ('scene', 0.0),\n",
       " ('scream', 0.0),\n",
       " ('screams', 0.0),\n",
       " ('self', 0.0),\n",
       " ('sensorsenso', 0.0),\n",
       " ('serious', 0.0),\n",
       " ('service', 0.0),\n",
       " ('sick', 0.0),\n",
       " ('sign', 0.0),\n",
       " ('sinkhole', 0.0),\n",
       " ('sirens', 0.0),\n",
       " ('site', 0.0),\n",
       " ('smoke', 0.0),\n",
       " ('soon', 0.0),\n",
       " ('sound', 0.0),\n",
       " ('spill', 0.0),\n",
       " ('st', 0.0),\n",
       " ('still', 0.0),\n",
       " ('such', 0.0),\n",
       " ('suicide', 0.0),\n",
       " ('summer', 0.0),\n",
       " ('sure', 0.0),\n",
       " ('suspect', 0.0),\n",
       " ('test', 0.0),\n",
       " ('thanks', 0.0),\n",
       " ('theres', 0.0),\n",
       " ('these', 0.0),\n",
       " ('thing', 0.0),\n",
       " ('those', 0.0),\n",
       " ('thunderstorm', 0.0),\n",
       " ('tornado', 0.0),\n",
       " ('totally', 0.0),\n",
       " ('traffic', 0.0),\n",
       " ('tragedy', 0.0),\n",
       " ('train', 0.0),\n",
       " ('trauma', 0.0),\n",
       " ('tree', 0.0),\n",
       " ('truck', 0.0),\n",
       " ('turkish', 0.0),\n",
       " ('turn', 0.0),\n",
       " ('two', 0.0),\n",
       " ('under', 0.0),\n",
       " ('united', 0.0),\n",
       " ('update', 0.0),\n",
       " ('upheaval', 0.0),\n",
       " ('use', 0.0),\n",
       " ('victim', 0.0),\n",
       " ('video', 0.0),\n",
       " ('warning', 0.0),\n",
       " ('watch', 0.0),\n",
       " ('wave', 0.0),\n",
       " ('weather', 0.0),\n",
       " ('whats', 0.0),\n",
       " ('whirlwind', 0.0),\n",
       " ('wild', 0.0),\n",
       " ('wonder', 0.0),\n",
       " ('wont', 0.0),\n",
       " ('wound', 0.0),\n",
       " ('yeah', 0.0),\n",
       " ('yet', 0.0),\n",
       " ('youth', 0.0),\n",
       " ('zone', 0.0),\n",
       " ('ûò', 0.0),\n",
       " ('possible', -0.012744901956190746),\n",
       " ('car', -0.013826865449400707),\n",
       " ('typhoon', -0.02440590619641599),\n",
       " ('up', -0.02451528671322521),\n",
       " ('storm', -0.03084336707275753),\n",
       " ('engulfed', -0.03200907914192049),\n",
       " ('room', -0.035036309972135915),\n",
       " ('calgary', -0.03685289871035553),\n",
       " ('officials', -0.03909615579964328),\n",
       " ('come', -0.04701766595879728),\n",
       " ('forget', -0.049025982076629115),\n",
       " ('west', -0.053467087646348284),\n",
       " ('20', -0.05361076559988423),\n",
       " ('buildings', -0.06472839252020544),\n",
       " ('human', -0.06815654861534444),\n",
       " ('today', -0.07469105145402732),\n",
       " ('12', -0.07813168494526336),\n",
       " ('release', -0.08425631222992493),\n",
       " ('camp', -0.08579997900620681),\n",
       " ('cry', -0.08668919687941831),\n",
       " ('steal', -0.08853981858231019),\n",
       " ('one', -0.09168941049566551),\n",
       " ('word', -0.0924375751752803),\n",
       " ('11', -0.09582906298690215),\n",
       " ('kid', -0.09716635462461262),\n",
       " ('grow', -0.10466893412367388),\n",
       " ('collision', -0.10473636268018385),\n",
       " ('night', -0.10762858977462353),\n",
       " ('bioterror', -0.1138999355538915),\n",
       " ('famine', -0.11497286552977323),\n",
       " ('drown', -0.12886164577547393),\n",
       " ('report', -0.13105791211663995),\n",
       " ('legionnaires', -0.1332726990448322),\n",
       " ('hazard', -0.1343870146550803),\n",
       " ('ask', -0.13552183621881336),\n",
       " ('all', -0.1394360892919138),\n",
       " ('pain', -0.14463480792768235),\n",
       " ('crash', -0.1572551373750407),\n",
       " ('team', -0.16160746906130274),\n",
       " ('door', -0.1672570909695904),\n",
       " ('pick', -0.16784756508989468),\n",
       " ('lose', -0.16836268092886444),\n",
       " ('class', -0.175376531334718),\n",
       " ('loud', -0.1913466174681112),\n",
       " ('siren', -0.20039069777234603),\n",
       " ('god', -0.20212512406581026),\n",
       " ('giant', -0.20251799498247666),\n",
       " ('mudslide', -0.2056815156187173),\n",
       " ('case', -0.21577248217276118),\n",
       " ('carry', -0.2257064676610386),\n",
       " ('peace', -0.2370029870060527),\n",
       " ('leave', -0.25197019689302097),\n",
       " ('nowplaying', -0.25257848463934535),\n",
       " ('hailstorm', -0.25821058522565515),\n",
       " ('trapped', -0.25893143323585266),\n",
       " ('boat', -0.2626942822927966),\n",
       " ('phone', -0.26520419879757473),\n",
       " ('ball', -0.2670199663027796),\n",
       " ('burn', -0.2681051146560146),\n",
       " ('gop', -0.26893886772238035),\n",
       " ('confirmed', -0.2732813907191646),\n",
       " ('bus', -0.2785959744236151),\n",
       " ('lightning', -0.28821266513865984),\n",
       " ('ship', -0.293073587944068),\n",
       " ('sunk', -0.2940345950091823),\n",
       " ('stop', -0.2983508944286882),\n",
       " ('libya', -0.3017364231864768),\n",
       " ('know', -0.31702517623699716),\n",
       " ('might', -0.3263024591540618),\n",
       " ('abc', -0.35390141477855325),\n",
       " ('pay', -0.35504401995745705),\n",
       " ('horrible', -0.3634808959750735),\n",
       " ('village', -0.3640191938665629),\n",
       " ('trust', -0.38626085847801284),\n",
       " ('ave', -0.39168109633163795),\n",
       " ('muslims', -0.39578177989875063),\n",
       " ('then', -0.4091782217003261),\n",
       " ('check', -0.4174063930391565),\n",
       " ('other', -0.4230998839562869),\n",
       " ('ass', -0.42578241026719565),\n",
       " ('usa', -0.4312738745458316),\n",
       " ('spot', -0.43707831015048565),\n",
       " ('shots', -0.4372787511270453),\n",
       " ('wind', -0.44752295042038204),\n",
       " ('york', -0.4487792921081473),\n",
       " ('live', -0.44946210010985743),\n",
       " ('emmerdale', -0.456886394437165),\n",
       " ('australia', -0.4631408001630438),\n",
       " ('long', -0.4638883096175394),\n",
       " ('group', -0.46984798261625954),\n",
       " ('outbreak', -0.4816294858477004),\n",
       " ('wildfire', -0.49082368038569685),\n",
       " ('fucking', -0.4957367792787327),\n",
       " ('center', -0.5014628467061459),\n",
       " ('least', -0.5027782285238306),\n",
       " ('every', -0.5044689307401071),\n",
       " ('show', -0.536910464298565),\n",
       " ('terror', -0.5403488791860349),\n",
       " ('fukushima', -0.5406311520782457),\n",
       " ('japanese', -0.5421606347421579),\n",
       " ('love', -0.5440005257633713),\n",
       " ('south', -0.5515751245052744),\n",
       " ('down', -0.5536687914048882),\n",
       " ('id', -0.5568483839176278),\n",
       " ('body', -0.564272149900864),\n",
       " ('over', -0.5699911631158331),\n",
       " ('music', -0.5912610217074851),\n",
       " ('prebreak', -0.5920226404351661),\n",
       " ('find', -0.6018377466489706),\n",
       " ('thank', -0.6036116363316926),\n",
       " ('burning', -0.6060051363180702),\n",
       " ('taiwan', -0.6087060859364164),\n",
       " ('collapse', -0.6101359899699109),\n",
       " ('texas', -0.6141230316513528),\n",
       " ('ruin', -0.6161063619561776),\n",
       " ('year', -0.6194229707180834),\n",
       " ('control', -0.6292802581189468),\n",
       " ('anyone', -0.635145472153887),\n",
       " ('cnn', -0.6437742119325036),\n",
       " ('boy', -0.6462869146832388),\n",
       " ('fuck', -0.6735022651471934),\n",
       " ('why', -0.674425774755262),\n",
       " ('bay', -0.6811905275093878),\n",
       " ('murder', -0.6851803877573913),\n",
       " ('forest', -0.6981313434510452),\n",
       " ('both', -0.7074493142195322),\n",
       " ('stretcher', -0.7146773990501097),\n",
       " ('confirm', -0.720465757576138),\n",
       " ('where', -0.7568249559913833),\n",
       " ('volcano', -0.7649045267024949),\n",
       " ('company', -0.7833959410388714),\n",
       " ('huge', -0.8266902455812144),\n",
       " ('obliterate', -0.8317419079144318),\n",
       " ('off', -0.840651295074691),\n",
       " ('national', -0.8409769247710227),\n",
       " ('learn', -0.8524708202439542),\n",
       " ('ebola', -0.8671285395237063),\n",
       " ('annihilate', -0.8751675154917712),\n",
       " ('terrorist', -0.9283147517114261),\n",
       " ('close', -0.9366466738347285),\n",
       " ('cake', -0.9368807379022428),\n",
       " ('need', -0.9480627850203952),\n",
       " ('survive', -0.9539904923954943),\n",
       " ('demolish', -0.9662521667675154),\n",
       " ('myanmar', -0.9791259268838657),\n",
       " ('survivor', -0.9806839325615451),\n",
       " ('blizzard', -0.9848413980397815),\n",
       " ('stay', -1.013956392451495),\n",
       " ('since', -1.0191363402321054),\n",
       " ('cover', -1.0551087896124962),\n",
       " ('swallowed', -1.0972757007908955),\n",
       " ('derail', -1.113531906521794),\n",
       " ('start', -1.1940886870568739),\n",
       " ('israel', -1.2104801651918973),\n",
       " ('blazing', -1.2138512044659293),\n",
       " ('past', -1.2406981075581764),\n",
       " ('warn', -1.409279562210166),\n",
       " ('iran', -1.5380864944941657),\n",
       " ('work', -1.5436384370181038),\n",
       " ('blight', -1.5718095920630266),\n",
       " ('top', -1.637327121799108),\n",
       " ('should', -1.6780616084214504),\n",
       " ('typhoondevastated', -2.222962080611733)]"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dict = dict()\n",
    "words = vtz.inverse_transform(clf_l1.coef_)[0]\n",
    "for i in range(len(words)):\n",
    "    param_dict[words[i]] = clf_l1.coef_[0][i]\n",
    "\n",
    "sorted_dict = sorted(param_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "2e2a7fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7859019264448336\n"
     ]
    }
   ],
   "source": [
    "# Bernoulli Naive Bayes\n",
    "\n",
    "def nb_predictions(x, psis, phis):\n",
    "    # adjust shapes\n",
    "    n, d = x.shape\n",
    "    x = np.reshape(x, (1, n, d))\n",
    "    psis = np.reshape(psis, (K, 1, d))\n",
    "    \n",
    "    # clip probabilities to avoid log(0)\n",
    "    psis = psis.clip(1e-14, 1-1e-14)\n",
    "    \n",
    "    # compute log-probabilities\n",
    "    logpy = np.log(phis).reshape([K,1])\n",
    "    logpxy = x * np.log(psis) + (1-x) * np.log(1-psis)\n",
    "    logpyx = logpxy.sum(axis=2) + logpy\n",
    "\n",
    "    return logpyx.argmax(axis=0).flatten(), logpyx.reshape([K,n])\n",
    "\n",
    "\n",
    "def Bernoulli_Naive_Bayes(xtrain, ytrain, xdev, K, alpha):\n",
    "    n = xtrain.shape[0]  # number of tweets\n",
    "    d = xtrain.shape[1]  # number of words in dataset\n",
    "    psis = np.zeros([K,d])\n",
    "    phis = np.zeros([K])\n",
    "\n",
    "    for k in range(K):\n",
    "        X_k = xtrain[ytrain == k]\n",
    "        psis[k] = (np.sum(X_k, axis=0) + alpha) / (X_k.shape[0] + 2 * alpha)\n",
    "        phis[k] = X_k.shape[0] / float(n)  \n",
    "\n",
    "    return nb_predictions(xdev, psis, phis)[0]\n",
    "    \n",
    "idx = Bernoulli_Naive_Bayes(V_train, y_train, V_dev, K = 2, alpha = 1)\n",
    "print(f1_score(idx, y_dev, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996eb4fd",
   "metadata": {},
   "source": [
    "model comparison here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "83bcae5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 nigerian\n",
      "15 saudi\n",
      "16yr old\n",
      "2015 prebreak\n",
      "40 family\n",
      "70 year\n",
      "add video\n",
      "affect fatal\n",
      "after waving\n",
      "air ambulance\n"
     ]
    }
   ],
   "source": [
    "# N-gram model\n",
    "\n",
    "M2 = 10\n",
    "vectorizer2 = CountVectorizer(binary=True, min_df=M2, ngram_range=(1,2))\n",
    "vtz2 = vectorizer2.fit(X_train['text'])\n",
    "V_train2 = vtz2.transform(X_train['text']).toarray()\n",
    "V_dev2 = vtz2.transform(X_dev['text']).toarray()\n",
    "\n",
    "features = vectorizer2.get_feature_names_out()\n",
    "counter = 0\n",
    "for feature in features:\n",
    "    if ' ' in feature and counter < 10:\n",
    "        print(feature)\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "7940ae59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining data: 0.811407543698252\n",
      "\tDevelopment data: 0.7268673355629878\n"
     ]
    }
   ],
   "source": [
    "clf_2gram = LogisticRegression(penalty='l2', solver='liblinear', max_iter=1000).fit(V_train2, y_train)\n",
    "y_train_predict = clf_2gram.predict(V_train2)\n",
    "f1_train = f1_score(y_train, y_train_predict)\n",
    "print('\\tTraining data:', f1_train)\n",
    "\n",
    "y_dev_predict = clf_2gram.predict(V_dev2)\n",
    "f1_dev = f1_score(y_dev, y_dev_predict)\n",
    "print('\\tDevelopment data:',f1_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0521f364",
   "metadata": {},
   "source": [
    "Logistic regression with L2 is chosen because it has highest F-score in development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "8ade116c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8018389941827735\n",
      "0.7898423817863398\n"
     ]
    }
   ],
   "source": [
    "idx_train = Bernoulli_Naive_Bayes(V_train2, y_train, V_train2, K = 2, alpha = 1)\n",
    "print(f1_score(idx_train, y_train, average='micro'))\n",
    "\n",
    "idx_dev = Bernoulli_Naive_Bayes(V_train2, y_train, V_dev2, K = 2, alpha = 1)\n",
    "print(f1_score(idx_dev, y_dev, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "e7c3adb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = preprocess_text(test['text'])\n",
    "train['text'] = preprocess_text(train['text'])\n",
    "\n",
    "M3 = 10\n",
    "vectorizer3 = CountVectorizer(binary=True, min_df=M3, ngram_range=(1,2))\n",
    "vtz3 = vectorizer3.fit(train['text'])\n",
    "V_train3 = vtz3.transform(train['text']).toarray()\n",
    "V_test = vtz3.transform(test['text']).toarray()\n",
    "idx_test = Bernoulli_Naive_Bayes(V_train3, train['target'], V_test, K = 2, alpha = 1)\n",
    "\n",
    "# Generates final df that is used for creating a csv file\n",
    "final_df = pd.DataFrame({'id': test['id'], 'target': idx_test})\n",
    "final_df.to_csv('disaster_predicted.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e99648",
   "metadata": {},
   "source": [
    "![output.png](output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250cf586",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
