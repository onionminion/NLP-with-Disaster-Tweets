{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef761591",
   "metadata": {},
   "source": [
    "# Homework 2 Binary Classification on Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ab9016",
   "metadata": {},
   "source": [
    "## Part a: Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c43fda68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20e7cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20a5a4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data points: 7613\n",
      "Test data points: 3263\n"
     ]
    }
   ],
   "source": [
    "print('Training data points:', len(train))\n",
    "print('Test data points:', len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac6c8f8",
   "metadata": {},
   "source": [
    "1) There are 7613 training data points and 3263 test data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2fefd8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4296597924602653"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[train['target'] == 1]) / len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb908310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5703402075397347"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[train['target'] == 0]) / len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd2156b",
   "metadata": {},
   "source": [
    "2) \\~43% of the training tweets are about real disasters and  \\~57% of the training tweets are about non-real disasters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbed273d",
   "metadata": {},
   "source": [
    "## Part b: Split the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5219f13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b1b2884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>686</td>\n",
       "      <td>attack</td>\n",
       "      <td>#UNITE THE BLUE</td>\n",
       "      <td>@blazerfan not everyone can see ignoranceshe i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4854</th>\n",
       "      <td>6913</td>\n",
       "      <td>mass%20murderer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>White people I know you worry tirelessly about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4270</th>\n",
       "      <td>6066</td>\n",
       "      <td>heat%20wave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chilli heat wave Doritos never fail!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>1441</td>\n",
       "      <td>body%20bagging</td>\n",
       "      <td>New Your</td>\n",
       "      <td>@BroseidonRex @dapurplesharpie I skimmed throu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4475</th>\n",
       "      <td>6365</td>\n",
       "      <td>hostages</td>\n",
       "      <td>cuba</td>\n",
       "      <td>#hot  C-130 specially modified to land in a st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4931</th>\n",
       "      <td>7025</td>\n",
       "      <td>mayhem</td>\n",
       "      <td>Manavadar, Gujarat</td>\n",
       "      <td>They are the real heroes... RIP Brave hearts.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3264</th>\n",
       "      <td>4689</td>\n",
       "      <td>engulfed</td>\n",
       "      <td>USA</td>\n",
       "      <td>Car engulfed in flames backs up traffic at Par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>2388</td>\n",
       "      <td>collapsed</td>\n",
       "      <td>Alexandria, Egypt.</td>\n",
       "      <td>Great British Bake Off's back and Dorret's cho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2607</th>\n",
       "      <td>3742</td>\n",
       "      <td>destroyed</td>\n",
       "      <td>USA</td>\n",
       "      <td>Black Eye 9: A space battle occurred at Star O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>3924</td>\n",
       "      <td>devastated</td>\n",
       "      <td>Dorset, UK</td>\n",
       "      <td>???????????? @MikeParrActor absolutely devasta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5329 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id          keyword            location  \\\n",
       "476    686           attack   #UNITE THE BLUE     \n",
       "4854  6913  mass%20murderer                 NaN   \n",
       "4270  6066      heat%20wave                 NaN   \n",
       "992   1441   body%20bagging            New Your   \n",
       "4475  6365         hostages                cuba   \n",
       "...    ...              ...                 ...   \n",
       "4931  7025           mayhem  Manavadar, Gujarat   \n",
       "3264  4689         engulfed                 USA   \n",
       "1653  2388        collapsed  Alexandria, Egypt.   \n",
       "2607  3742        destroyed                 USA   \n",
       "2732  3924       devastated          Dorset, UK   \n",
       "\n",
       "                                                   text  \n",
       "476   @blazerfan not everyone can see ignoranceshe i...  \n",
       "4854  White people I know you worry tirelessly about...  \n",
       "4270               Chilli heat wave Doritos never fail!  \n",
       "992   @BroseidonRex @dapurplesharpie I skimmed throu...  \n",
       "4475  #hot  C-130 specially modified to land in a st...  \n",
       "...                                                 ...  \n",
       "4931  They are the real heroes... RIP Brave hearts.....  \n",
       "3264  Car engulfed in flames backs up traffic at Par...  \n",
       "1653  Great British Bake Off's back and Dorret's cho...  \n",
       "2607  Black Eye 9: A space battle occurred at Star O...  \n",
       "2732  ???????????? @MikeParrActor absolutely devasta...  \n",
       "\n",
       "[5329 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X = list(train.columns[:-1])\n",
    "\n",
    "# Splits train.csv into training set (70%) and development set (30%)\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(train[X], train['target'], test_size=0.3, random_state=0)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b47840c",
   "metadata": {},
   "source": [
    "## Part c: Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3ab183",
   "metadata": {},
   "source": [
    "To clean noise and unprocessed content, we did the following:\n",
    "* Convert all the words to lowercase\n",
    "* Remove all URLs\n",
    "* Removes usernames (e.g. @twitter)\n",
    "* Strip punctuations\n",
    "* Strip stop words\n",
    "* Lemmatize words based on part of speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bd7600",
   "metadata": {},
   "source": [
    "We converted all the words to lowercase because the first letter of the first word in a sentence is usually capitalized. Moreover, text could be in all caps. \n",
    "\n",
    "We removed all URLs because URLs are mostly sequences of meaningless characters and do not contain valuable information.\n",
    "\n",
    "We removed Twitter usernames because usernames do not provide valuable information about the text itself.\n",
    "\n",
    "We stripped punctuations because not everyone uses punctuations when they tweet.\n",
    "\n",
    "We generated our own set of stop words and stripped them from the text. Stop words are commonly used words that do not provide much information about the text. \n",
    "\n",
    "Lastly, we lemmatized words based on the part of speech so that we don't have different variations of the same word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92145aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "stop_words = ['the', 'a', 'an', 'and', 'or', 'this', 'that', 'i', 'my', 'me', 'we', 'us', 'our', 'she', 'her', \n",
    "              'he', 'his', 'him', 'they', 'their', 'them', 'you', 'your', 'there', 'are', 'is', 'from', 'to',\n",
    "              'will', 'can', 'cant', 'must', 'might', 'should', 'shouldnt', 'would', 'wouldnt', 'has', 'hasnt', \n",
    "              'have', 'havent', 'could', 'couldnt', 'as', 'if', 'in', 'on', 'also', 'at', 'such', 'under', 'lol',\n",
    "              'lmao', 'haha', 'of', 'into', 'over', 'with', 'by', 'be', 'it', 'its', 'so', 'im', 'youre', 'theyre', \n",
    "              'hes', 'shes', 'were', 'was', 'not', 'but', 'no', 'never', 'with', 'really', 'do', 'does', 'dont', \n",
    "              'doesnt', 'for', 'about', 'what', 'how', 'who', 'just', 'when', 'via', 'which', 'than', 'yes', \n",
    "              'yeah', 'up', 'more', 'most', 'isnt', 'arent', 'am', 'wont', 'werent', 'wasnt', 'yet']\n",
    "\n",
    "def regex_stop_word(words):\n",
    "    regex = r'\\b'\n",
    "    for i in range(len(words)):\n",
    "        if i == len(words) - 1:\n",
    "            regex += words[i] + r'\\b'\n",
    "        else:\n",
    "            regex += words[i] + r'\\b|\\b'\n",
    "    return regex\n",
    "\n",
    "\n",
    "def preprocess_text(X):\n",
    "    # Converts all the words to lowercase\n",
    "    X = X.apply(lambda text: text.lower())\n",
    "    \n",
    "    # Removes URLs\n",
    "    X = X.apply(lambda text: re.sub(r'http\\S+', ' ', text))\n",
    "    \n",
    "    # Removes user id\n",
    "    X = X.apply(lambda text: re.sub(r'@(.*?)[\\s]', ' ', text))\n",
    "    \n",
    "    # Strips punctuations\n",
    "    X = X.apply(lambda text: text.replace('-', ' '))\n",
    "    X = X.apply(lambda text: re.sub(r'[^\\w\\s]', '', text))\n",
    "    \n",
    "    # Strips stop words\n",
    "    X = X.apply(lambda text: re.sub(regex_stop_word(stop_words), ' ', text))\n",
    "    return X\n",
    "\n",
    "X_train['text'] = preprocess_text(X_train['text'])\n",
    "X_dev['text'] = preprocess_text(X_dev['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fef4e9f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "476     everyone see ignoranceshe latinoand all ever b...\n",
       "4854    white people know worry tirelessly black black...\n",
       "4270                        chilli heat wave doritos fail\n",
       "992                    skim through twitter miss body bag\n",
       "4475    hot c 130 specially modified land stadium resc...\n",
       "                              ...                        \n",
       "4931                            real hero rip brave heart\n",
       "3264      car engulf flames back traffic parleyûªs summit\n",
       "1653    great british bake offs back dorrets chocolate...\n",
       "2607    black eye 9 space battle occur star o784 invol...\n",
       "2732    absolutely devastate actor miss rossbarton eve...\n",
       "Name: text, Length: 5329, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet \n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# import nltk\n",
    "# nltk.download()\n",
    "\n",
    "# Lemmatize words based on part of speech (verbs, adjectives, and nouns)\n",
    "def lemmatize(text):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    word_tags = pos_tag(text.split())\n",
    "    result_text = []\n",
    "    for word_tag in word_tags:\n",
    "        lemmatized_word = word_tag[0]\n",
    "        # lemmatize verbs (e.g. ate -> eat)\n",
    "        if 'VB' in word_tag[1]:\n",
    "            lemmatized_word = wnl.lemmatize(word_tag[0], pos='v')\n",
    "        # lemmatize adjectives (e.g. better -> good)\n",
    "        elif 'JJ' in word_tag[1]:\n",
    "            lemmatized_word = wnl.lemmatize(word_tag[0], pos='a')\n",
    "        # lemmatize nouns (e.g. cookies -> cookie)\n",
    "        elif 'NN' in word_tag[1]:\n",
    "            lemmatized_word = wnl.lemmatize(word_tag[0], pos='n')\n",
    "        result_text.append(lemmatized_word)\n",
    "    return ' '.join(result_text)\n",
    "\n",
    "X_train['text'] = X_train['text'].apply(lambda text: lemmatize(text))\n",
    "X_dev['text'] = X_dev['text'].apply(lambda text: lemmatize(text))\n",
    "test['text'] = test['text'].apply(lambda text: lemmatize(text))\n",
    "X_train['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f7574a",
   "metadata": {},
   "source": [
    "## Part d: Bag of words model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b631ab",
   "metadata": {},
   "source": [
    "The threshold selected for our bag of words model is 10. In other words, our model includes only the words that appear in at least 10 different tweets. \n",
    "\n",
    "We selected 10 as our threshold by evaluating the performance of our logistic regression model on the development set with different thresholds as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fa44d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_dev(M, xtrain, ytrain, xdev, ydev, ngram_range=(1, 1)):\n",
    "    vectorizer = CountVectorizer(binary=True, min_df=M, ngram_range=ngram_range)\n",
    "    vtz = vectorizer.fit(xtrain)\n",
    "    V_train = vtz.transform(xtrain).toarray()\n",
    "    V_dev = vtz.transform(xdev).toarray()\n",
    "    \n",
    "    clf = LogisticRegression(solver='saga', penalty='l1', max_iter=3000).fit(V_train, ytrain)\n",
    "    y_dev_predict = clf.predict(V_dev)\n",
    "    f1_dev = f1_score(ydev, y_dev_predict)\n",
    "    print('When M = ', M , ', F1 score = ', f1_dev, sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c0352b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When M = 10, F1 score = 0.7383230163196398\n",
      "When M = 15, F1 score = 0.7226318774815654\n",
      "When M = 20, F1 score = 0.7163841807909606\n"
     ]
    }
   ],
   "source": [
    "f1_score_dev(10, X_train['text'], y_train, X_dev['text'], y_dev)\n",
    "f1_score_dev(15, X_train['text'], y_train, X_dev['text'], y_dev)\n",
    "f1_score_dev(20, X_train['text'], y_train, X_dev['text'], y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a62bc0",
   "metadata": {},
   "source": [
    "When M < 10, we ran into a run-time problem and were forced to choose a higher threshold.\n",
    "\n",
    "From the F1 scores above, we concluded that M = 10 prunes the tweets just enough to provide good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce8462e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10\n",
    "vectorizer = CountVectorizer(binary=True, min_df=M)\n",
    "vtz = vectorizer.fit(X_train['text'])\n",
    "V_train = vtz.transform(X_train['text']).toarray()\n",
    "V_dev = vtz.transform(X_dev['text']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7226325c",
   "metadata": {},
   "source": [
    "## Part e: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "213a1c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_f1(penalty):\n",
    "    clf = LogisticRegression(penalty=penalty, solver='saga', max_iter = 3000).fit(V_train, y_train)\n",
    "    y_train_predict = clf.predict(V_train)\n",
    "    f1_train = f1_score(y_train, y_train_predict)\n",
    "\n",
    "    y_dev_predict = clf.predict(V_dev)\n",
    "    f1_dev = f1_score(y_dev, y_dev_predict)\n",
    "    \n",
    "    return clf, f1_train, f1_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4524188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores for logistic regression model without regularization\n",
      "\tTraining data: 0.839213934792318\n",
      "\tDevelopment data: 0.7191854233654876\n"
     ]
    }
   ],
   "source": [
    "print('F1 scores for logistic regression model without regularization')\n",
    "(clf_none, f1_train, f1_dev) = logistic_regression_f1('none')\n",
    "print('\\tTraining data:', f1_train)\n",
    "print('\\tDevelopment data:', f1_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53cee41",
   "metadata": {},
   "source": [
    "The F1 score in the training data (\\~0.839) is significantly higher than the F1 score in the development data (\\~0.719). This means that our model performs significantly better in the training data than it does in the development data. Therefore, our logistic regression model without regularization terms is overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f6a9d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores for logistic regression model with L1 regularization\n",
      "\tTraining data: 0.8068886337543054\n",
      "\tDevelopment data: 0.7383230163196398\n"
     ]
    }
   ],
   "source": [
    "print('F1 scores for logistic regression model with L1 regularization')\n",
    "(clf_l1, f1_train, f1_dev) = logistic_regression_f1('l1')\n",
    "print('\\tTraining data:', f1_train)\n",
    "print('\\tDevelopment data:', f1_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f67a4502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores for logistic regression model with L2 regularization\n",
      "\tTraining data: 0.8101033295063145\n",
      "\tDevelopment data: 0.7340067340067339\n"
     ]
    }
   ],
   "source": [
    "print('F1 scores for logistic regression model with L2 regularization')\n",
    "(clf_l2, f1_train, f1_dev) = logistic_regression_f1('l2')\n",
    "print('\\tTraining data:', f1_train)\n",
    "print('\\tDevelopment data:', f1_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f858604",
   "metadata": {},
   "source": [
    "Among the three logistic regression models, the logistic regression model without regularization terms performed the best on the training data with an F1 score of \\~0.839. However, the logistic regression model with L1 regularization performed the best on the development data with an F1 score of \\~0.738. \n",
    "\n",
    "The difference between the F1 score in the training data and the F1 score in the development data for the logistic regression model without regularization terms is \\~0.12. The difference in the F1 scores for the model with L1 regularization is \\~0.069. The difference in the F1 scores for the model with L2 regularization is \\~0.076. For the two models with regularization, the difference seems to be smaller than that of the model without any regularization terms. Moreover, the F1 scores in the develpment data for the models with regularization are higher than that of the model without regularization. Therefore, we can conclude that regularization helped reduce overfitting in our logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed30bbfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('want', 3.475688038344507),\n",
       " ('volcano', 3.46635726633739),\n",
       " ('israeli', 3.380727444405068),\n",
       " ('room', 3.335617758856058),\n",
       " ('hawaii', 2.5861837792880955),\n",
       " ('south', 2.4807738184294217),\n",
       " ('typhoon', 2.389501443468807),\n",
       " ('fukushima', 2.3549279419512614),\n",
       " ('outrage', 2.232281767077627),\n",
       " ('disaster', 2.172872233570794),\n",
       " ('guess', 2.1592033713344563),\n",
       " ('life', 2.1256429031574267),\n",
       " ('injure', 2.0971829222855236),\n",
       " ('say', 2.0290459384373354),\n",
       " ('bioterror', 1.983683331345891),\n",
       " ('wound', 1.9545517458472794),\n",
       " ('york', 1.8926315634504545),\n",
       " ('due', 1.7874767006845993),\n",
       " ('st', 1.71725297935092),\n",
       " ('issue', 1.7163804347651577)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect weight vector of classifier with L1 regularization\n",
    "\n",
    "param_dict = dict()\n",
    "words = vtz.inverse_transform(clf_l1.coef_)[0]\n",
    "for i in range(len(words)):\n",
    "    param_dict[words[i]] = clf_l1.coef_[0][i]\n",
    "\n",
    "sorted_dict = sorted(param_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_dict[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2252e3c1",
   "metadata": {},
   "source": [
    "The words above are some of the most important words for deciding whether a tweet is about a real disaster or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c109794c",
   "metadata": {},
   "source": [
    "## Part f: Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e2a7fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.792907180385289\n"
     ]
    }
   ],
   "source": [
    "# Bernoulli Naive Bayes\n",
    "\n",
    "def nb_predictions(x, psis, phis, K):\n",
    "    # adjust shapes\n",
    "    n, d = x.shape\n",
    "    x = np.reshape(x, (1, n, d))\n",
    "    psis = np.reshape(psis, (K, 1, d))\n",
    "    \n",
    "    # clip probabilities to avoid log(0)\n",
    "    psis = psis.clip(1e-14, 1-1e-14)\n",
    "    \n",
    "    # compute log-probabilities\n",
    "    logpy = np.log(phis).reshape([K,1])\n",
    "    logpxy = x * np.log(psis) + (1-x) * np.log(1-psis)\n",
    "    logpyx = logpxy.sum(axis=2) + logpy\n",
    "\n",
    "    return logpyx.argmax(axis=0).flatten(), logpyx.reshape([K,n])\n",
    "\n",
    "\n",
    "def Bernoulli_Naive_Bayes(xtrain, ytrain, xdev, K, alpha):\n",
    "    n = xtrain.shape[0]  # number of tweets\n",
    "    d = xtrain.shape[1]  # number of words in dataset\n",
    "    psis = np.zeros([K,d])\n",
    "    phis = np.zeros([K])\n",
    "\n",
    "    for k in range(K):\n",
    "        X_k = xtrain[ytrain == k]\n",
    "        psis[k] = (np.sum(X_k, axis=0) + alpha) / (X_k.shape[0] + 2 * alpha)\n",
    "        phis[k] = X_k.shape[0] / float(n)  \n",
    "\n",
    "    return nb_predictions(xdev, psis, phis, K)[0]\n",
    "    \n",
    "idx = Bernoulli_Naive_Bayes(V_train, y_train, V_dev, K = 2, alpha = 1)\n",
    "print(f1_score(idx, y_dev, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd15e356",
   "metadata": {},
   "source": [
    "The F1 score on the development set for our Bernoulli Naive Bayes classifier is \\~0.793 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68e9d58",
   "metadata": {},
   "source": [
    "## Part g: Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996eb4fd",
   "metadata": {},
   "source": [
    "The F1 scores on the development set for our four classifiers are the following:\n",
    "\n",
    "* Bernoulli Naive Bayes: \\~0.793\n",
    "* Logistic Regression with L1 Regularization: \\~0.738\n",
    "* Logistic Regression with L2 Regularization: \\~0.734\n",
    "* Logistic Regression withouot Regularization: \\~0.719\n",
    "\n",
    "The Bernoulli Naive Bayes model performed the best in predicting whether a tweet is of a real disaster or not, with an F1 score of \\~0.793."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ae55d4",
   "metadata": {},
   "source": [
    "One advantage of using generative models is that we do not need to worry about dealing with missing values and noisy inputs with generative models. Moreover, the maximum likelihood parameters are fairly simple for generative models. However, generative models might not perform well if the assumptions made for building the models are not true. \n",
    "\n",
    "One advantage of using discriminative models is that they are often more accurate than generative models because they are based on fewer assumptions than generative models. On the other hand, the accuracy of the discriminative models could be low when we have inputs with missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a699f9c5",
   "metadata": {},
   "source": [
    "The Bernoulli Naive Bayes model assumes that each word in the text is independent, meaning that for each word and each class, the probability of observing that word within that class can be represented using a single number. However, for the logistic regression, the conditional probability is used for the words instead, meaning that some correlations in the words have a minimal effect on the performance.\n",
    "\n",
    "Since not all the words are present in each Tweet, it is efficient to use a Bernoulli Naive Bayes classifier for natural language texts. We do not need to deal with missing words when we use a Bernoulli Naive Bayes classifier. It is also efficient as it uses simple formulas to calculate maximum likelihood parameters. However, if the words in a sentence have high correlations with each other, our initial assumption for the Naive Bayes classifier might not hold true and thus, it would not perform well.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efdf120",
   "metadata": {},
   "source": [
    "## Part h: N-gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377875fb",
   "metadata": {},
   "source": [
    "The threshold selected for the 2-gram model is 10. Our 2-gram model includes only the words that appear in at least 10 different tweets. \n",
    "\n",
    "We selected 10 as our threshold by evaluating the performance of our logistic regression model with L1 regularization on the development set with different thresholds as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e88c85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When M = 10, F1 score = 0.7320113314447593\n",
      "When M = 11, F1 score = 0.7293318233295584\n",
      "When M = 12, F1 score = 0.7319004524886878\n",
      "When M = 13, F1 score = 0.7250996015936255\n",
      "When M = 14, F1 score = 0.7241576242147345\n",
      "When M = 15, F1 score = 0.7246871444823663\n",
      "When M = 20, F1 score = 0.7178329571106093\n"
     ]
    }
   ],
   "source": [
    "f1_score_dev(10, X_train['text'], y_train, X_dev['text'], y_dev, (1,2))\n",
    "f1_score_dev(11, X_train['text'], y_train, X_dev['text'], y_dev, (1,2))\n",
    "f1_score_dev(12, X_train['text'], y_train, X_dev['text'], y_dev, (1,2))\n",
    "f1_score_dev(13, X_train['text'], y_train, X_dev['text'], y_dev, (1,2))\n",
    "f1_score_dev(14, X_train['text'], y_train, X_dev['text'], y_dev, (1,2))\n",
    "f1_score_dev(15, X_train['text'], y_train, X_dev['text'], y_dev, (1,2))\n",
    "f1_score_dev(20, X_train['text'], y_train, X_dev['text'], y_dev, (1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90b7e63",
   "metadata": {},
   "source": [
    "Again, when M < 10, we ran into a run-time problem and were forced to choose a higher threshold.\n",
    "From the F1 scores above, we concluded that M = 10 prunes the tweets just enough to provide good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83bcae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-gram set up\n",
    "\n",
    "M2 = 10\n",
    "vectorizer2 = CountVectorizer(binary=True, min_df=M2, ngram_range=(1,2))\n",
    "vtz2 = vectorizer2.fit(X_train['text'])\n",
    "V_train2 = vtz2.transform(X_train['text']).toarray()\n",
    "V_dev2 = vtz2.transform(X_dev['text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cd6a76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08 05\n",
      "12000 nigerian\n",
      "15 saudi\n",
      "16yr old\n",
      "2015 prebreak\n",
      "40 family\n",
      "70 year\n",
      "add video\n",
      "affect fatal\n",
      "after exchange\n"
     ]
    }
   ],
   "source": [
    "# Print 10 2-grams from vocabulary\n",
    "features = vectorizer2.get_feature_names_out()\n",
    "counter = 0\n",
    "for feature in features:\n",
    "    if counter < 10:\n",
    "        if ' ' in feature:\n",
    "            print(feature)\n",
    "            counter += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b42308",
   "metadata": {},
   "source": [
    "The words listed above are some of the 2-grams from the vocabulary of our 2-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9dbc311a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1038 1-grams\n",
      "244 2-grams\n"
     ]
    }
   ],
   "source": [
    "# Number of 1-grams and 2-grams\n",
    "onegram_counter = 0\n",
    "twogram_counter = 0\n",
    "for feature in features:\n",
    "    if ' ' in feature:\n",
    "        twogram_counter += 1\n",
    "    else:\n",
    "        onegram_counter += 1\n",
    "print(onegram_counter, '1-grams')\n",
    "print(twogram_counter, '2-grams')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c674b9",
   "metadata": {},
   "source": [
    "For our 2-gram model, there are 1038 1-grams and 244 2-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0521f364",
   "metadata": {},
   "source": [
    "We generated a logistic regression model with L1 regularization on our 2-gram.\n",
    "\n",
    "Logistic regression with L1 is chosen because it had the highest F-score in development set for 2-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7940ae59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining data: 0.8068234209313048\n",
      "\tDevelopment data: 0.7320113314447593\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression with L1 regularization on 2-gram\n",
    "\n",
    "clf_2gram = LogisticRegression(penalty='l1', solver='saga', max_iter=3000).fit(V_train2, y_train)\n",
    "y_train_predict = clf_2gram.predict(V_train2)\n",
    "f1_train = f1_score(y_train, y_train_predict)\n",
    "print('\\tTraining data:', f1_train)\n",
    "\n",
    "y_dev_predict = clf_2gram.predict(V_dev2)\n",
    "f1_dev = f1_score(y_dev, y_dev_predict)\n",
    "print('\\tDevelopment data:',f1_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed52b3c4",
   "metadata": {},
   "source": [
    "Then, we generated a Bernoulli classifier on our 2-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ade116c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining data: 0.8087821354850816\n",
      "\tDevelopment data: 0.7946584938704028\n"
     ]
    }
   ],
   "source": [
    "# Bernoulli classifier on 2-gram\n",
    "\n",
    "idx_train = Bernoulli_Naive_Bayes(V_train2, y_train, V_train2, K = 2, alpha = 1)\n",
    "print('\\tTraining data:', f1_score(idx_train, y_train, average='micro'))\n",
    "\n",
    "idx_dev = Bernoulli_Naive_Bayes(V_train2, y_train, V_dev2, K = 2, alpha = 1)\n",
    "print('\\tDevelopment data:', f1_score(idx_dev, y_dev, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eab0684",
   "metadata": {},
   "source": [
    "The F1 score for the logistic regression model with L1 regularization using the bags of words on the development set was \\~0.738 and the F1 score for the logistic regression model using the 2-grams on the development set was \\~0.732. The difference in these results is only \\~0.006, meaning that they do not differ significantly.\n",
    "\n",
    "The F1 score for the Bernoulli classifier using the bags of words on the development set was \\~0.793 and the F1 score for the Bernoulli classifier using the 2-grams on the development set was \\~0.795. The difference in these results is only \\~0.002, meaning that they do not differ significantly, again.\n",
    "\n",
    "The small difference in these results implies that 2-grams do not play much role in our data. This might mean that consecutive words in our text data are mostly independent, resulting in similar results to the bag of words model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0fee24",
   "metadata": {},
   "source": [
    "## Part i: Determine performance with the test set "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2271e4ff",
   "metadata": {},
   "source": [
    "The final model that we settled on was the Bernoulli classifier using 2-grams. We trained this classifier on the entire training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7c3adb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = preprocess_text(test['text'])\n",
    "train['text'] = preprocess_text(train['text'])\n",
    "\n",
    "M3 = 10\n",
    "vectorizer3 = CountVectorizer(binary=True, min_df=M3, ngram_range=(1,2))\n",
    "vtz3 = vectorizer3.fit(train['text'])\n",
    "V_train3 = vtz3.transform(train['text']).toarray()\n",
    "V_test = vtz3.transform(test['text']).toarray()\n",
    "idx_test = Bernoulli_Naive_Bayes(V_train3, train['target'], V_test, K = 2, alpha = 1)\n",
    "\n",
    "# Generates final df that is used for creating a csv file\n",
    "final_df = pd.DataFrame({'id': test['id'], 'target': idx_test})\n",
    "final_df.to_csv('disaster_predicted.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e99648",
   "metadata": {},
   "source": [
    "![output.png](output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fe5d1b",
   "metadata": {},
   "source": [
    "The F1 score on the test data is 0.77321. \n",
    "\n",
    "We expected the test data F1 score to be very close to the development data F1 score for the Bernoulli classifier using the 2-grams because the development data F1 score was very close to that of the training data. In other words, we expected the F1 score for the test data to be close to 0.795. The actual F1 score came out to be slightly lower than our expectation, meaning that our model could be slightly overfitting. This might be due to an increase in the volume of vocabulary used in our classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
